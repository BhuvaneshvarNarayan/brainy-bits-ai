{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7475c5-9ec9-4d3b-9e2e-74b591d59325",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The following cell has the emotion detection algorithm\")\n",
    "# UNIT 1, Working perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T21:30:01.934237Z",
     "start_time": "2024-06-23T21:29:43.974938Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install cv2\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    if faces:\n",
    "        shape = predictor(gray, faces[0])\n",
    "        left_eye = shape.parts()[36:42]\n",
    "        right_eye = shape.parts()[42:48]\n",
    "        return left_eye, right_eye\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point.x, point.y) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"/scripts/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model('/scripts/video.h5')\n",
    "\n",
    "emotion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/results/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = 0\n",
    "duration_looking_left = 0\n",
    "duration_looking_right = 0\n",
    "duration_looking_straight = 0\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = 0\n",
    "count_right = 0\n",
    "count_straight = 0\n",
    "\n",
    "# Load face detector and shape predictor for emotion detection\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "shape_predictor = dlib.shape_predictor(\"/scripts/face_landmarks.dat\")\n",
    "\n",
    "# Initialize head pose estimation\n",
    "official_start_time = time.time()\n",
    "start_time = time.time()\n",
    "end_time = 0\n",
    "\n",
    "#Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "e_start_time = time.time()\n",
    "e_end_time = 0\n",
    "angry_emotion = 0\n",
    "sad_emotion = 0\n",
    "happy_emotion = 0\n",
    "fear_emotion = 0\n",
    "disgust_emotion = 0\n",
    "neutral_emotion = 0\n",
    "surprise_emotion = 0\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = 0\n",
    "time_left_seconds = 0\n",
    "time_right_seconds = 0\n",
    "time_up_seconds = 0\n",
    "time_down_seconds = 0\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Eye tracking\n",
    "    left_eye, right_eye = detect_eyes(frame)\n",
    "\n",
    "    if left_eye is not None and right_eye is not None:\n",
    "        ear_left = calculate_ear(left_eye)\n",
    "        ear_right = calculate_ear(right_eye)\n",
    "\n",
    "        # Calculate the average EAR for both eyes\n",
    "        avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "        # Set a threshold for distraction detection (you may need to adjust this)\n",
    "        distraction_threshold = 0.2\n",
    "\n",
    "        # Check if the person is distracted\n",
    "        if avg_ear < distraction_threshold:\n",
    "            cv2.putText(frame, \"Eyes Closed\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "            duration_eyes_closed += 1 / fps  # Increment the duration\n",
    "            count_straight += 1\n",
    "\n",
    "        else:\n",
    "            # Check gaze direction\n",
    "            horizontal_ratio = (left_eye[0].x + right_eye[3].x) / 2 / width\n",
    "            if horizontal_ratio < 0.4:\n",
    "                cv2.putText(frame, \"Looking Left\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_left += 1 / fps  # Increment the duration\n",
    "                count_left += 1\n",
    "            elif horizontal_ratio > 0.6:\n",
    "                cv2.putText(frame, \"Looking Right\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_right += 1 / fps  # Increment the duration\n",
    "                count_right += 1\n",
    "            else:\n",
    "                cv2.putText(frame, \"Looking Straight\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_straight += 1 / fps  # Increment the duration\n",
    "\n",
    "        # Draw contours around eyes\n",
    "        for eye in [left_eye, right_eye]:\n",
    "            for point in eye:\n",
    "                x, y = point.x, point.y\n",
    "                cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "    # Emotion detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = face_detector(gray, 1)\n",
    "\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = shape_predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        face = gray[y:y + h, x:x + w]\n",
    "        face = zoom(face, (48 / face.shape[0], 48 / face.shape[1]))\n",
    "        face = face.astype(np.float32)\n",
    "        face /= float(face.max())\n",
    "        face = np.reshape(face.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "        prediction = emotion_model.predict(face)\n",
    "        prediction_result = np.argmax(prediction)\n",
    "\n",
    "        # Rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Annotate main image with emotion label\n",
    "        if prediction_result == 0:\n",
    "            cv2.putText(frame, \"Angry\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            angry_emotion += time.time() - e_start_time\n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 1:\n",
    "            cv2.putText(frame, \"Disgust\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            disgust_emotion += time.time() - e_start_time\n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 2:\n",
    "            cv2.putText(frame, \"Fear\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            fear_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 3:\n",
    "            cv2.putText(frame, \"Happy\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            happy_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 4:\n",
    "            cv2.putText(frame, \"Sad\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            sad_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 5:\n",
    "            cv2.putText(frame, \"Surprise\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            surprise_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        else:\n",
    "            cv2.putText(frame, \"Neutral\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            neutral_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "\n",
    "    # Head pose estimation\n",
    "    startTime = time.time()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #it was 1\n",
    "#     frame = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "    frame.flags.writeable = False\n",
    "    results = face_mesh.process(frame)\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    img_h, img_w, img_c = frame.shape\n",
    "    face_3d = []\n",
    "    face_2d = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                    if idx == 1:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "\n",
    "            face_2d = np.array(face_2d, dtype=np.float64)\n",
    "            face_3d = np.array(face_3d, dtype=np.float64)\n",
    "            focal_length = 1 * img_w\n",
    "\n",
    "            cam_matrix = np.array([[focal_length, 0, img_h / 2],\n",
    "                                   [0, focal_length, img_w / 2],\n",
    "                                   [0, 0, 1]])\n",
    "\n",
    "            dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "            rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "            print(f\"X Rotation: {angles[0] * 10000}\")\n",
    "            print(f\"Y Rotation: {angles[1] * 10000}\")\n",
    "\n",
    "            if angles[1] * 10000 < -100:\n",
    "                text = \"Looking Left\"\n",
    "                time_left_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[1] * 10000 > 100:\n",
    "                text = \"Looking Right\"\n",
    "                time_right_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[0] * 10000 < -100:\n",
    "                text = \"Looking Down\"\n",
    "                time_down_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[0] * 10000 > 200:\n",
    "                text = \"Looking Up\"\n",
    "                time_up_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            else:\n",
    "                text = \"Forward\"\n",
    "                time_forward_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            # Display the nose direction\n",
    "            nose_3d_projection, jacobian = cv2.projectPoints(nose_3d, rot_vec, trans_vec, cam_matrix, dist_matrix)\n",
    "\n",
    "            p1 = (int(nose_2d[0]), int(nose_2d[1]))\n",
    "            p2 = (int(nose_3d_projection[0][0][0]), int(nose_3d_projection[0][0][1]))\n",
    "\n",
    "            cv2.line(frame, p1, p2, (255, 0, 0), 2)\n",
    "\n",
    "            cv2.putText(frame, text, (width - 250, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "\n",
    "    # Open the CSV file in write mode and append the angles to it\n",
    "    with open('headPoses.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the header row if the file is empty\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"X Rotation\", \"Y Rotation\"])\n",
    "\n",
    "        # Write the angles to the CSV file\n",
    "        #writer.writerow([angles[0] * 10000, angles[1] * 10000]) #bonbon\n",
    "\n",
    "    output_video.write(frame)  # Write the frame to the output video\n",
    "\n",
    "    # Display the frame without modifying color\n",
    "    cv2.imshow('Frame', frame)\n",
    "    # Clear the previous output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object, video writer, and close all windows\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Print the durations and most observed features for emotion detection\n",
    "print(f\"\\nEmotion Detection:\")\n",
    "print(f\"Duration of Happiness: {happy_emotion} seconds\")\n",
    "print(f\"Duration of Sadness: {sad_emotion} seconds\")\n",
    "print(f\"Duration of Disgust: {disgust_emotion} seconds\")\n",
    "print(f\"Duration of Fear: {fear_emotion} seconds\")\n",
    "print(f\"Duration of Anger: {angry_emotion} seconds\")\n",
    "print(f\"Duration of Neutral: {neutral_emotion} seconds\")\n",
    "print(f\"Duration of Surprise: {surprise_emotion} seconds\")\n",
    "\n",
    "# Determine the most observed emotions movement\n",
    "max_eye_duration = max(happy_emotion, sad_emotion, disgust_emotion, fear_emotion, angry_emotion, neutral_emotion, surprise_emotion)\n",
    "if max_eye_duration == happy_emotion:\n",
    "    print(\"The most observed emotion: Happiness\")\n",
    "elif max_eye_duration == sad_emotion:\n",
    "    print(\"The most observed emotion: Sadness\")\n",
    "elif max_eye_duration == disgust_emotion:\n",
    "    print(\"The most observed emotion: Disgust\")\n",
    "elif max_eye_duration == fear_emotion:\n",
    "    print(\"The most observed emotion: Fear\")\n",
    "elif max_eye_duration == angry_emotion:\n",
    "    print(\"The most observed emotion: Anger\")\n",
    "elif max_eye_duration == surprise_emotion:\n",
    "    print(\"The most observed emotion: Surprise\")\n",
    "else:\n",
    "    print(\"The most observed emotion: Neutral\")\n",
    "\n",
    "\n",
    "# Print the durations and most observed features for eyes\n",
    "print(f\"\\nEye Movements:\")\n",
    "print(f\"Duration taken looking right: {duration_looking_right} sec\")\n",
    "print(f\"Duration taken closed eyes: {duration_eyes_closed} sec\")\n",
    "print(f\"Duration taken looking left: {duration_looking_left} sec\")\n",
    "print(f\"Duration taken looking straight: {duration_looking_straight} sec\")\n",
    "\n",
    "# Determine the most observed eye movement\n",
    "max_eye_duration = max(duration_looking_right, duration_eyes_closed, duration_looking_left, duration_looking_straight)\n",
    "if max_eye_duration == duration_looking_right:\n",
    "    print(\"The most observed eye movement: Looking Right\")\n",
    "elif max_eye_duration == duration_eyes_closed:\n",
    "    print(\"The most observed eye movement: Eyes Closed\")\n",
    "elif max_eye_duration == duration_looking_left:\n",
    "    print(\"The most observed eye movement: Looking Left\")\n",
    "else:\n",
    "    print(\"The most observed eye movement: Looking Straight\")\n",
    "\n",
    "# Print the durations and most observed features for head pose\n",
    "print(f\"\\nHead Pose Estimation:\")\n",
    "print(f\"Duration of Time Looking Forward: {time_forward_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Up: {time_up_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Left: q{time_left_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Right: {time_right_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Down: {time_down_seconds} seconds\")\n",
    "\n",
    "# Determine the most observed eye movement\n",
    "max_eye_duration = max(time_forward_seconds, time_up_seconds, time_left_seconds, time_right_seconds, time_down_seconds)\n",
    "if max_eye_duration == time_forward_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Forward\")\n",
    "elif max_eye_duration == time_up_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Upwards\")\n",
    "elif max_eye_duration == time_left_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Left\")\n",
    "elif max_eye_duration == time_right_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Right\")\n",
    "else:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Downwards\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8812bfdb7783986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The following cell has multiple face detection algorithm\")\n",
    "# UNIT 2, Working perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2fd3b33d18729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Initialize the video capture (use 0 for the first camera device)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize dlib's face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "person_count = 1\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the grayscale frame\n",
    "    faces = detector(gray)\n",
    "\n",
    "    # Loop through each detected face\n",
    "    for i, face in enumerate(faces):\n",
    "        # Draw a rectangle around the face\n",
    "        x, y, w, h = (face.left(), face.top(), face.width(), face.height())\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # Label the face with a unique identifier\n",
    "        label = f'Person {i+1}'\n",
    "        cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Face Detection', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4f9447fe1678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL, Test success\n",
    "print(\"The following cell has the integrated code which is final one of version 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8bd206-752f-4206-a47c-a881219c9b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python-headless in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from opencv-python-headless) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dlib in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (19.24.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipython in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (8.25.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from ipython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from ipython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from ipython) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from ipython) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from ipython) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from ipython) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from ipython) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from asttokens>=2.1.0->stack-data->ipython) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from scipy) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imutils in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (0.5.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python3117\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mediapipe in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (0.10.14)\n",
      "Requirement already satisfied: absl-py in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (3.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from jax->mediapipe) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nisha\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open /scripts/shape_predictor_68_face_landmarks.dat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Load dlib face detector and facial landmarks predictor\u001b[39;00m\n\u001b[0;32m     49\u001b[0m detector \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mget_frontal_face_detector()\n\u001b[1;32m---> 50\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mdlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/scripts/shape_predictor_68_face_landmarks.dat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Load emotion detection model\u001b[39;00m\n\u001b[0;32m     53\u001b[0m emotion_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/scripts/FER_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to open /scripts/shape_predictor_68_face_landmarks.dat"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python-headless\n",
    "!pip install dlib\n",
    "!pip install numpy\n",
    "!pip install ipython\n",
    "!pip install scipy\n",
    "!pip install imutils\n",
    "!pip install tensorflow\n",
    "!pip install mediapipe\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "\n",
    "# Custom deserialization function for VarianceScaling\n",
    "def custom_VarianceScaling_deserializer(config):\n",
    "    from tensorflow.keras.initializers import VarianceScaling\n",
    "    # Remove 'dtype' from config if it exists\n",
    "    config.pop('dtype', None)\n",
    "    return VarianceScaling(**config)\n",
    "\n",
    "# Register the custom deserializer\n",
    "tf.keras.utils.get_custom_objects().update({'VarianceScaling': custom_VarianceScaling_deserializer})\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame, shape):\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "    return left_eye, right_eye\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point[0], point[1]) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"/scripts/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model('/scripts/FER_model.h5')\n",
    "\n",
    "emotion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/results/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = {}\n",
    "duration_looking_left = {}\n",
    "duration_looking_right = {}\n",
    "duration_looking_straight = {}\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = {}\n",
    "count_right = {}\n",
    "count_straight = {}\n",
    "\n",
    "# Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "emotion_duration = {\"angry\": {}, \"sad\": {}, \"happy\": {}, \"fear\": {}, \"disgust\": {}, \"neutral\": {}, \"surprise\": {}}\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = {}\n",
    "time_left_seconds = {}\n",
    "time_right_seconds = {}\n",
    "time_up_seconds = {}\n",
    "time_down_seconds = {}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        person_id = f\"Person {i+1}\"\n",
    "\n",
    "        if person_id not in duration_eyes_closed:\n",
    "            duration_eyes_closed[person_id] = 0\n",
    "            duration_looking_left[person_id] = 0\n",
    "            duration_looking_right[person_id] = 0\n",
    "            duration_looking_straight[person_id] = 0\n",
    "            count_left[person_id] = 0\n",
    "            count_right[person_id] = 0\n",
    "            count_straight[person_id] = 0\n",
    "            time_forward_seconds[person_id] = 0\n",
    "            time_left_seconds[person_id] = 0\n",
    "            time_right_seconds[person_id] = 0\n",
    "            time_up_seconds[person_id] = 0\n",
    "            time_down_seconds[person_id] = 0\n",
    "            for emotion in emotion_duration:\n",
    "                emotion_duration[emotion][person_id] = 0\n",
    "\n",
    "        # Eye tracking\n",
    "        left_eye, right_eye = detect_eyes(frame, shape)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "            # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "            # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, f\"{person_id}: Eyes Closed\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "                duration_eyes_closed[person_id] += 1 / fps  # Increment the duration\n",
    "                count_straight[person_id] += 1\n",
    "\n",
    "            else:\n",
    "                # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0][0] + right_eye[3][0]) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Left\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_left[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_left[person_id] += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Right\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_right[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_right[person_id] += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Straight\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_straight[person_id] += 1 / fps  # Increment the duration\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point[0], point[1]\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Emotion detection\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        face_crop = gray[y:y + h, x:x + w]\n",
    "        face_crop = zoom(face_crop, (48 / face_crop.shape[0], 48 / face_crop.shape[1]))\n",
    "        face_crop = face_crop.astype(np.float32)\n",
    "        face_crop /= float(face_crop.max())\n",
    "        face_crop = np.reshape(face_crop.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "        prediction = emotion_model.predict(face_crop)\n",
    "        prediction_result = np.argmax(prediction)\n",
    "\n",
    "        # Rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Annotate main image with emotion label\n",
    "        emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "        emotion_label = emotion_labels[prediction_result]\n",
    "        cv2.putText(frame, f\"{person_id}: {emotion_label}\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        emotion_duration[emotion_label.lower()][person_id] += time.time() - emotion_start_time\n",
    "        emotion_start_time = time.time()\n",
    "\n",
    "        # Head pose estimation\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = frame.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "          for face_landmarks in results.multi_face_landmarks:\n",
    "              for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                  if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                      if idx == 1:\n",
    "                          nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                          nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "                      x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                      # Get the 2D Coordinates\n",
    "                      face_2d.append([x, y])\n",
    "\n",
    "                      # Get the 3D Coordinates\n",
    "                      face_3d.append([x, y, lm.z])\n",
    "\n",
    "          face_2d = np.array(face_2d, dtype=np.float64)\n",
    "          face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "          # Camera matrix\n",
    "          focal_length = 1 * img_w\n",
    "          cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                [0, focal_length, img_h / 2],\n",
    "                                [0, 0, 1]])\n",
    "\n",
    "          # Distortion parameters\n",
    "          dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "          # Solve PnP\n",
    "          success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "          # Get rotational matrix\n",
    "          rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "          # Get angles\n",
    "          angles, mtx_r, mtx_q, qx, qy, qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "          # Get the y rotation degree\n",
    "          x_angle = angles[0] * 360\n",
    "          y_angle = angles[1] * 360\n",
    "          z_angle = angles[2] * 360\n",
    "\n",
    "          # See where the user's head tilting\n",
    "          if y_angle < -10:\n",
    "              text = \"Looking Left\"\n",
    "              time_left_seconds[person_id] += 1 / fps\n",
    "          elif y_angle > 10:\n",
    "              text = \"Looking Right\"\n",
    "              time_right_seconds[person_id] += 1 / fps\n",
    "          elif x_angle < -10:\n",
    "              text = \"Looking Down\"\n",
    "              time_down_seconds[person_id] += 1 / fps\n",
    "          elif x_angle > 10:\n",
    "              text = \"Looking Up\"\n",
    "              time_up_seconds[person_id] += 1 / fps\n",
    "          else:\n",
    "              text = \"Looking Forward\"\n",
    "              time_forward_seconds[person_id] += 1 / fps\n",
    "\n",
    "          # Display the text\n",
    "          cv2.putText(frame, f\"{person_id}: {text}\", (500, 50 + i * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Clear the output in Jupyter notebook\n",
    "    clear_output(wait=True)\n",
    "    display(frame)\n",
    "\n",
    "    # Exit the loop when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "with open('/results/eye_tracking_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Duration Eyes Closed (s)\", \"Duration Looking Left (s)\", \"Duration Looking Right (s)\", \"Duration Looking Straight (s)\", \"Left Counts\", \"Right Counts\", \"Straight Counts\"])\n",
    "    for person_id in duration_eyes_closed:\n",
    "        writer.writerow([person_id, duration_eyes_closed[person_id], duration_looking_left[person_id], duration_looking_right[person_id], duration_looking_straight[person_id], count_left[person_id], count_right[person_id], count_straight[person_id]])\n",
    "\n",
    "with open('/results/emotion_detection_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Angry (s)\", \"Sad (s)\", \"Happy (s)\", \"Fear (s)\", \"Disgust (s)\", \"Neutral (s)\", \"Surprise (s)\"])\n",
    "    for person_id in emotion_duration[\"angry\"]:\n",
    "        writer.writerow([person_id, emotion_duration[\"angry\"][person_id], emotion_duration[\"sad\"][person_id], emotion_duration[\"happy\"][person_id], emotion_duration[\"fear\"][person_id], emotion_duration[\"disgust\"][person_id], emotion_duration[\"neutral\"][person_id], emotion_duration[\"surprise\"][person_id]])\n",
    "\n",
    "with open('/results/head_pose_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Looking Forward (s)\", \"Looking Left (s)\", \"Looking Right (s)\", \"Looking Up (s)\", \"Looking Down (s)\"])\n",
    "    for person_id in time_forward_seconds:\n",
    "        writer.writerow([person_id, time_forward_seconds[person_id], time_left_seconds[person_id], time_right_seconds[person_id], time_up_seconds[person_id], time_down_seconds[person_id]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f859b5-7510-4e5c-b858-c2394020996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code 1\n",
    "print(\"Simple code to read the .h5 file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20e00f-ded7-4ead-8813-7a3413f01d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Path to the HDF5 file\n",
    "h5_file_path = 'video_modified.h5'\n",
    "\n",
    "# Open the HDF5 file for reading\n",
    "with h5py.File(h5_file_path, 'r') as f:\n",
    "    print(\"Keys in the HDF5 file:\")\n",
    "    print(\"======================\")\n",
    "    # Print all groups and datasets\n",
    "    def print_attrs(name, obj):\n",
    "        print(name)\n",
    "        for key, val in obj.attrs.items():\n",
    "            print(\"    %s: %s\" % (key, val))\n",
    "\n",
    "    f.visititems(print_attrs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
