{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc756ef-3f19-4a9b-afcb-89c0dd1d6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cv2\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    if faces:\n",
    "        shape = predictor(gray, faces[0])\n",
    "        left_eye = shape.parts()[36:42]\n",
    "        right_eye = shape.parts()[42:48]\n",
    "        return left_eye, right_eye\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point.x, point.y) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/video.h5\")\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/content/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = 0\n",
    "duration_looking_left = 0\n",
    "duration_looking_right = 0\n",
    "duration_looking_straight = 0\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = 0\n",
    "count_right = 0\n",
    "count_straight = 0\n",
    "\n",
    "# Load face detector and shape predictor for emotion detection\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "shape_predictor = dlib.shape_predictor(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/face_landmarks.dat\")\n",
    "\n",
    "# Initialize head pose estimation\n",
    "official_start_time = time.time()\n",
    "start_time = time.time()\n",
    "end_time = 0\n",
    "\n",
    "#Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "e_start_time = time.time()\n",
    "e_end_time = 0\n",
    "angry_emotion = 0\n",
    "sad_emotion = 0\n",
    "happy_emotion = 0\n",
    "fear_emotion = 0\n",
    "disgust_emotion = 0\n",
    "neutral_emotion = 0\n",
    "surprise_emotion = 0\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = 0\n",
    "time_left_seconds = 0\n",
    "time_right_seconds = 0\n",
    "time_up_seconds = 0\n",
    "time_down_seconds = 0\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Eye tracking\n",
    "    left_eye, right_eye = detect_eyes(frame)\n",
    "\n",
    "    if left_eye is not None and right_eye is not None:\n",
    "        ear_left = calculate_ear(left_eye)\n",
    "        ear_right = calculate_ear(right_eye)\n",
    "\n",
    "        # Calculate the average EAR for both eyes\n",
    "        avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "        # Set a threshold for distraction detection (you may need to adjust this)\n",
    "        distraction_threshold = 0.2\n",
    "\n",
    "        # Check if the person is distracted\n",
    "        if avg_ear < distraction_threshold:\n",
    "            cv2.putText(frame, \"Eyes Closed\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "            duration_eyes_closed += 1 / fps  # Increment the duration\n",
    "            count_straight += 1\n",
    "\n",
    "        else:\n",
    "            # Check gaze direction\n",
    "            horizontal_ratio = (left_eye[0].x + right_eye[3].x) / 2 / width\n",
    "            if horizontal_ratio < 0.4:\n",
    "                cv2.putText(frame, \"Looking Left\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_left += 1 / fps  # Increment the duration\n",
    "                count_left += 1\n",
    "            elif horizontal_ratio > 0.6:\n",
    "                cv2.putText(frame, \"Looking Right\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_right += 1 / fps  # Increment the duration\n",
    "                count_right += 1\n",
    "            else:\n",
    "                cv2.putText(frame, \"Looking Straight\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_straight += 1 / fps  # Increment the duration\n",
    "\n",
    "        # Draw contours around eyes\n",
    "        for eye in [left_eye, right_eye]:\n",
    "            for point in eye:\n",
    "                x, y = point.x, point.y\n",
    "                cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "    # Emotion detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = face_detector(gray, 1)\n",
    "\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = shape_predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        face = gray[y:y + h, x:x + w]\n",
    "        face = zoom(face, (48 / face.shape[0], 48 / face.shape[1]))\n",
    "        face = face.astype(np.float32)\n",
    "        face /= float(face.max())\n",
    "        face = np.reshape(face.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "        prediction = emotion_model.predict(face)\n",
    "        prediction_result = np.argmax(prediction)\n",
    "\n",
    "        # Rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Annotate main image with emotion label\n",
    "        if prediction_result == 0:\n",
    "            cv2.putText(frame, \"Angry\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            angry_emotion += time.time() - e_start_time\n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 1:\n",
    "            cv2.putText(frame, \"Disgust\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            disgust_emotion += time.time() - e_start_time\n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 2:\n",
    "            cv2.putText(frame, \"Fear\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            fear_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 3:\n",
    "            cv2.putText(frame, \"Happy\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            happy_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 4:\n",
    "            cv2.putText(frame, \"Sad\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            sad_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 5:\n",
    "            cv2.putText(frame, \"Surprise\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            surprise_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        else:\n",
    "            cv2.putText(frame, \"Neutral\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            neutral_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "\n",
    "    # Head pose estimation\n",
    "    startTime = time.time()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #it was 1\n",
    "#     frame = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "    frame.flags.writeable = False\n",
    "    results = face_mesh.process(frame)\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    img_h, img_w, img_c = frame.shape\n",
    "    face_3d = []\n",
    "    face_2d = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                    if idx == 1:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "\n",
    "            face_2d = np.array(face_2d, dtype=np.float64)\n",
    "            face_3d = np.array(face_3d, dtype=np.float64)\n",
    "            focal_length = 1 * img_w\n",
    "\n",
    "            cam_matrix = np.array([[focal_length, 0, img_h / 2],\n",
    "                                   [0, focal_length, img_w / 2],\n",
    "                                   [0, 0, 1]])\n",
    "\n",
    "            dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "            rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "            print(f\"X Rotation: {angles[0] * 10000}\")\n",
    "            print(f\"Y Rotation: {angles[1] * 10000}\")\n",
    "\n",
    "            if angles[1] * 10000 < -100:\n",
    "                text = \"Looking Left\"\n",
    "                time_left_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[1] * 10000 > 100:\n",
    "                text = \"Looking Right\"\n",
    "                time_right_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[0] * 10000 < -100:\n",
    "                text = \"Looking Down\"\n",
    "                time_down_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[0] * 10000 > 200:\n",
    "                text = \"Looking Up\"\n",
    "                time_up_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            else:\n",
    "                text = \"Forward\"\n",
    "                time_forward_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            # Display the nose direction\n",
    "            nose_3d_projection, jacobian = cv2.projectPoints(nose_3d, rot_vec, trans_vec, cam_matrix, dist_matrix)\n",
    "\n",
    "            p1 = (int(nose_2d[0]), int(nose_2d[1]))\n",
    "            p2 = (int(nose_3d_projection[0][0][0]), int(nose_3d_projection[0][0][1]))\n",
    "\n",
    "            cv2.line(frame, p1, p2, (255, 0, 0), 2)\n",
    "\n",
    "            cv2.putText(frame, text, (width - 250, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "\n",
    "    # Open the CSV file in write mode and append the angles to it\n",
    "    with open('headPoses.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the header row if the file is empty\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"X Rotation\", \"Y Rotation\"])\n",
    "\n",
    "        # Write the angles to the CSV file\n",
    "        #writer.writerow([angles[0] * 10000, angles[1] * 10000]) #bonbon\n",
    "\n",
    "    output_video.write(frame)  # Write the frame to the output video\n",
    "\n",
    "    # Display the frame without modifying color\n",
    "    cv2.imshow('Frame', frame)\n",
    "    # Clear the previous output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object, video writer, and close all windows\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Print the durations and most observed features for emotion detection\n",
    "print(f\"\\nEmotion Detection:\")\n",
    "print(f\"Duration of Happiness: {happy_emotion} seconds\")\n",
    "print(f\"Duration of Sadness: {sad_emotion} seconds\")\n",
    "print(f\"Duration of Disgust: {disgust_emotion} seconds\")\n",
    "print(f\"Duration of Fear: {fear_emotion} seconds\")\n",
    "print(f\"Duration of Anger: {angry_emotion} seconds\")\n",
    "print(f\"Duration of Neutral: {neutral_emotion} seconds\")\n",
    "print(f\"Duration of Surprise: {surprise_emotion} seconds\")\n",
    "\n",
    "# Determine the most observed emotions movement\n",
    "max_eye_duration = max(happy_emotion, sad_emotion, disgust_emotion, fear_emotion, angry_emotion, neutral_emotion, surprise_emotion)\n",
    "if max_eye_duration == happy_emotion:\n",
    "    print(\"The most observed emotion: Happiness\")\n",
    "elif max_eye_duration == sad_emotion:\n",
    "    print(\"The most observed emotion: Sadness\")\n",
    "elif max_eye_duration == disgust_emotion:\n",
    "    print(\"The most observed emotion: Disgust\")\n",
    "elif max_eye_duration == fear_emotion:\n",
    "    print(\"The most observed emotion: Fear\")\n",
    "elif max_eye_duration == angry_emotion:\n",
    "    print(\"The most observed emotion: Anger\")\n",
    "elif max_eye_duration == surprise_emotion:\n",
    "    print(\"The most observed emotion: Surprise\")\n",
    "else:\n",
    "    print(\"The most observed emotion: Neutral\")\n",
    "\n",
    "\n",
    "# Print the durations and most observed features for eyes\n",
    "print(f\"\\nEye Movements:\")\n",
    "print(f\"Duration taken looking right: {duration_looking_right} sec\")\n",
    "print(f\"Duration taken closed eyes: {duration_eyes_closed} sec\")\n",
    "print(f\"Duration taken looking left: {duration_looking_left} sec\")\n",
    "print(f\"Duration taken looking straight: {duration_looking_straight} sec\")\n",
    "\n",
    "# Determine the most observed eye movement\n",
    "max_eye_duration = max(duration_looking_right, duration_eyes_closed, duration_looking_left, duration_looking_straight)\n",
    "if max_eye_duration == duration_looking_right:\n",
    "    print(\"The most observed eye movement: Looking Right\")\n",
    "elif max_eye_duration == duration_eyes_closed:\n",
    "    print(\"The most observed eye movement: Eyes Closed\")\n",
    "elif max_eye_duration == duration_looking_left:\n",
    "    print(\"The most observed eye movement: Looking Left\")\n",
    "else:\n",
    "    print(\"The most observed eye movement: Looking Straight\")\n",
    "\n",
    "# Print the durations and most observed features for head pose\n",
    "print(f\"\\nHead Pose Estimation:\")\n",
    "print(f\"Duration of Time Looking Forward: {time_forward_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Up: {time_up_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Left: q{time_left_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Right: {time_right_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Down: {time_down_seconds} seconds\")\n",
    "\n",
    "# Determine the most observed eye movement\n",
    "max_eye_duration = max(time_forward_seconds, time_up_seconds, time_left_seconds, time_right_seconds, time_down_seconds)\n",
    "if max_eye_duration == time_forward_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Forward\")\n",
    "elif max_eye_duration == time_up_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Upwards\")\n",
    "elif max_eye_duration == time_left_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Left\")\n",
    "elif max_eye_duration == time_right_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Right\")\n",
    "else:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Downwards\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373094c8-0a29-4d05-b2af-000739669547",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cv2\n",
    "!pip install dlib\n",
    "!pip install numpy\n",
    "!pip install ipython\n",
    "!pip install scipy\n",
    "!pip install imutils\n",
    "!pip install tensorflow\n",
    "!pip install mediapipe\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame, shape):\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "    return left_eye, right_eye\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point[0], point[1]) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/video.h5\")\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/content/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = {}\n",
    "duration_looking_left = {}\n",
    "duration_looking_right = {}\n",
    "duration_looking_straight = {}\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = {}\n",
    "count_right = {}\n",
    "count_straight = {}\n",
    "\n",
    "# Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "emotion_duration = {\"angry\": {}, \"sad\": {}, \"happy\": {}, \"fear\": {}, \"disgust\": {}, \"neutral\": {}, \"surprise\": {}}\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = {}\n",
    "time_left_seconds = {}\n",
    "time_right_seconds = {}\n",
    "time_up_seconds = {}\n",
    "time_down_seconds = {}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        person_id = f\"Person {i+1}\"\n",
    "\n",
    "        if person_id not in duration_eyes_closed:\n",
    "            duration_eyes_closed[person_id] = 0\n",
    "            duration_looking_left[person_id] = 0\n",
    "            duration_looking_right[person_id] = 0\n",
    "            duration_looking_straight[person_id] = 0\n",
    "            count_left[person_id] = 0\n",
    "            count_right[person_id] = 0\n",
    "            count_straight[person_id] = 0\n",
    "            time_forward_seconds[person_id] = 0\n",
    "            time_left_seconds[person_id] = 0\n",
    "            time_right_seconds[person_id] = 0\n",
    "            time_up_seconds[person_id] = 0\n",
    "            time_down_seconds[person_id] = 0\n",
    "            for emotion in emotion_duration:\n",
    "                emotion_duration[emotion][person_id] = 0\n",
    "\n",
    "        # Eye tracking\n",
    "        left_eye, right_eye = detect_eyes(frame, shape)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "            # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "            # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, f\"{person_id}: Eyes Closed\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "                duration_eyes_closed[person_id] += 1 / fps  # Increment the duration\n",
    "                count_straight[person_id] += 1\n",
    "\n",
    "            else:\n",
    "                # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0][0] + right_eye[3][0]) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Left\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_left[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_left[person_id] += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Right\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_right[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_right[person_id] += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Straight\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_straight[person_id] += 1 / fps  # Increment the duration\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point[0], point[1]\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Emotion detection\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        face_crop = gray[y:y + h, x:x + w]\n",
    "        face_crop = zoom(face_crop, (48 / face_crop.shape[0], 48 / face_crop.shape[1]))\n",
    "        face_crop = face_crop.astype(np.float32)\n",
    "        face_crop /= float(face_crop.max())\n",
    "        face_crop = np.reshape(face_crop.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "        prediction = emotion_model.predict(face_crop)\n",
    "        prediction_result = np.argmax(prediction)\n",
    "\n",
    "        # Rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Annotate main image with emotion label\n",
    "        emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "        emotion_label = emotion_labels[prediction_result]\n",
    "        cv2.putText(frame, f\"{person_id}: {emotion_label}\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        emotion_duration[emotion_label.lower()][person_id] += time.time() - emotion_start_time\n",
    "        emotion_start_time = time.time()\n",
    "\n",
    "        # Head pose estimation\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = frame.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                    if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                        if idx == 1:\n",
    "                            nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                            nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                        x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                        # Get the 2D Coordinates\n",
    "                        face_2d.append([x, y])\n",
    "\n",
    "                        # Get\n",
    "                        # Get the 3D Coordinates\n",
    "                        face_3d.append([x, y, lm.z])\n",
    "\n",
    "                face_2d = np.array(face_2d, dtype=np.float64)\n",
    "                face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "                # Camera matrix\n",
    "                focal_length = 1 * img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                       [0, focal_length, img_h / 2],\n",
    "                                       [0, 0, 1]])\n",
    "\n",
    "                # Distortion parameters\n",
    "                dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "                # Solve PnP\n",
    "                success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "                # Get rotational matrix\n",
    "                rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "                # Get angles\n",
    "                angles, mtx_r, mtx_q, qx, qy, qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "                # Get the y rotation degree\n",
    "                x_angle = angles[0] * 360\n",
    "                y_angle = angles[1] * 360\n",
    "                z_angle = angles[2] * 360\n",
    "\n",
    "                # See where the user's head tilting\n",
    "                if y_angle < -10:\n",
    "                    text = \"Looking Left\"\n",
    "                    time_left_seconds[person_id] += 1 / fps\n",
    "                elif y_angle > 10:\n",
    "                    text = \"Looking Right\"\n",
    "                    time_right_seconds[person_id] += 1 / fps\n",
    "                elif x_angle < -10:\n",
    "                    text = \"Looking Down\"\n",
    "                    time_down_seconds[person_id] += 1 / fps\n",
    "                elif x_angle > 10:\n",
    "                    text = \"Looking Up\"\n",
    "                    time_up_seconds[person_id] += 1 / fps\n",
    "                else:\n",
    "                    text = \"Looking Forward\"\n",
    "                    time_forward_seconds[person_id] += 1 / fps\n",
    "\n",
    "                # Display the text\n",
    "                cv2.putText(frame, f\"{person_id}: {text}\", (500, 50 + i * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Clear the output in Jupyter notebook\n",
    "    clear_output(wait=True)\n",
    "    display(frame)\n",
    "\n",
    "    # Exit the loop when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Write the recorded data to a CSV file\n",
    "with open('eye_tracking_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Duration Eyes Closed (s)\", \"Duration Looking Left (s)\", \"Duration Looking Right (s)\", \"Duration Looking Straight (s)\", \"Left Counts\", \"Right Counts\", \"Straight Counts\"])\n",
    "    for person_id in duration_eyes_closed:\n",
    "        writer.writerow([person_id, duration_eyes_closed[person_id], duration_looking_left[person_id], duration_looking_right[person_id], duration_looking_straight[person_id], count_left[person_id], count_right[person_id], count_straight[person_id]])\n",
    "\n",
    "with open('emotion_detection_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Angry (s)\", \"Sad (s)\", \"Happy (s)\", \"Fear (s)\", \"Disgust (s)\", \"Neutral (s)\", \"Surprise (s)\"])\n",
    "    for person_id in emotion_duration[\"angry\"]:\n",
    "        writer.writerow([person_id, emotion_duration[\"angry\"][person_id], emotion_duration[\"sad\"][person_id], emotion_duration[\"happy\"][person_id], emotion_duration[\"fear\"][person_id], emotion_duration[\"disgust\"][person_id], emotion_duration[\"neutral\"][person_id], emotion_duration[\"surprise\"][person_id]])\n",
    "\n",
    "with open('head_pose_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Looking Forward (s)\", \"Looking Left (s)\", \"Looking Right (s)\", \"Looking Up (s)\", \"Looking Down (s)\"])\n",
    "    for person_id in time_forward_seconds:\n",
    "        writer.writerow([person_id, time_forward_seconds[person_id], time_left_seconds[person_id], time_right_seconds[person_id], time_up_seconds[person_id], time_down_seconds[person_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3ea9e-719e-4e12-958f-5cbd5b5a7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user opencv-contrib-python ml-dtypes sounddevice jaxlib jax mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f49d73-37bd-4059-9e90-594c63d2b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame, shape):\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "    return left_eye, right_eye\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point[0], point[1]) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/video.h5\")\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/content/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = {}\n",
    "duration_looking_left = {}\n",
    "duration_looking_right = {}\n",
    "duration_looking_straight = {}\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = {}\n",
    "count_right = {}\n",
    "count_straight = {}\n",
    "\n",
    "# Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "emotion_duration = {\"angry\": {}, \"sad\": {}, \"happy\": {}, \"fear\": {}, \"disgust\": {}, \"neutral\": {}, \"surprise\": {}}\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = {}\n",
    "time_left_seconds = {}\n",
    "time_right_seconds = {}\n",
    "time_up_seconds = {}\n",
    "time_down_seconds = {}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        person_id = f\"Person {i+1}\"\n",
    "\n",
    "        if person_id not in duration_eyes_closed:\n",
    "            duration_eyes_closed[person_id] = 0\n",
    "            duration_looking_left[person_id] = 0\n",
    "            duration_looking_right[person_id] = 0\n",
    "            duration_looking_straight[person_id] = 0\n",
    "            count_left[person_id] = 0\n",
    "            count_right[person_id] = 0\n",
    "            count_straight[person_id] = 0\n",
    "            time_forward_seconds[person_id] = 0\n",
    "            time_left_seconds[person_id] = 0\n",
    "            time_right_seconds[person_id] = 0\n",
    "            time_up_seconds[person_id] = 0\n",
    "            time_down_seconds[person_id] = 0\n",
    "            for emotion in emotion_duration:\n",
    "                emotion_duration[emotion][person_id] = 0\n",
    "\n",
    "        # Eye tracking\n",
    "        left_eye, right_eye = detect_eyes(frame, shape)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "            # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "            # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, f\"{person_id}: Eyes Closed\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "                duration_eyes_closed[person_id] += 1 / fps  # Increment the duration\n",
    "                count_straight[person_id] += 1\n",
    "\n",
    "            else:\n",
    "                # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0][0] + right_eye[3][0]) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Left\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_left[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_left[person_id] += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Right\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_right[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_right[person_id] += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Straight\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_straight[person_id] += 1 / fps  # Increment the duration\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point[0], point[1]\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Emotion detection\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        if w > 0 and h > 0:  # Check if width and height are positive\n",
    "            face_crop = gray[y:y + h, x:x + w]\n",
    "            if face_crop.shape[0] > 0 and face_crop.shape[1] > 0:  # Check if face crop dimensions are positive\n",
    "                face_crop = zoom(face_crop, (48 / face_crop.shape[0], 48 / face_crop.shape[1]))\n",
    "                face_crop = face_crop.astype(np.float32)\n",
    "                face_crop /= float(face_crop.max())\n",
    "                face_crop = np.reshape(face_crop.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "                prediction = emotion_model.predict(face_crop)\n",
    "                prediction_result = np.argmax(prediction)\n",
    "\n",
    "                # Rectangle around the face\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                # Annotate main image with emotion label\n",
    "                emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "                emotion_label = emotion_labels[prediction_result]\n",
    "                cv2.putText(frame, f\"{person_id}: {emotion_label}\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                emotion_duration[emotion_label.lower()][person_id] += time.time() - emotion_start_time\n",
    "                emotion_start_time = time.time()\n",
    "\n",
    "        # Head pose estimation\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = frame.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                    if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "\n",
    "                face_2d = np.array(face_2d, dtype=np.float64)\n",
    "                face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "                # Camera matrix\n",
    "                focal_length = 1 * img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                       [0, focal_length, img_h / 2],\n",
    "                                       [0, 0, 1]])\n",
    "\n",
    "                # Distortion parameters\n",
    "                dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "                # Solve PnP\n",
    "                success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "                # Get rotational matrix\n",
    "                rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "                # Get angles\n",
    "                angles, mtx_r, mtx_q, qx, qy, qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "                # Get the y rotation degree\n",
    "                x_angle = angles[0] * 360\n",
    "                y_angle = angles[1] * 360\n",
    "                z_angle = angles[2] * 360\n",
    "\n",
    "                # See where the user's head tilting\n",
    "                if y_angle < -10:\n",
    "                    text = \"Looking Left\"\n",
    "                    time_left_seconds[person_id] += 1 / fps\n",
    "                elif y_angle > 10:\n",
    "                    text = \"Looking Right\"\n",
    "                    time_right_seconds[person_id] += 1 / fps\n",
    "                elif x_angle < -10:\n",
    "                    text = \"Looking Down\"\n",
    "                    time_down_seconds[person_id] += 1 / fps\n",
    "                elif x_angle > 10:\n",
    "                    text = \"Looking Up\"\n",
    "                    time_up_seconds[person_id] += 1 / fps\n",
    "                else:\n",
    "                    text = \"Looking Forward\"\n",
    "                    time_forward_seconds[person_id] += 1 / fps\n",
    "\n",
    "                # Display the text\n",
    "                cv2.putText(frame, f\"{person_id}: {text}\", (500, 50 + i * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Clear the output in Jupyter notebook\n",
    "    clear_output(wait=True)\n",
    "    display(frame)\n",
    "\n",
    "    # Exit the loop when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Write the recorded data to a CSV file\n",
    "with open('eye_tracking_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Duration Eyes Closed (s)\", \"Duration Looking Left (s)\", \"Duration Looking Right (s)\", \"Duration Looking Straight (s)\", \"Left Counts\", \"Right Counts\", \"Straight Counts\"])\n",
    "    for person_id in duration_eyes_closed:\n",
    "        writer.writerow([person_id, duration_eyes_closed[person_id], duration_looking_left[person_id], duration_looking_right[person_id], duration_looking_straight[person_id], count_left[person_id], count_right[person_id], count_straight[person_id]])\n",
    "\n",
    "with open('emotion_detection_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Angry (s)\", \"Sad (s)\", \"Happy (s)\", \"Fear (s)\", \"Disgust (s)\", \"Neutral (s)\", \"Surprise (s)\"])\n",
    "    for person_id in emotion_duration[\"angry\"]:\n",
    "        writer.writerow([person_id, emotion_duration[\"angry\"][person_id], emotion_duration[\"sad\"][person_id], emotion_duration[\"happy\"][person_id], emotion_duration[\"fear\"][person_id], emotion_duration[\"disgust\"][person_id], emotion_duration[\"neutral\"][person_id], emotion_duration[\"surprise\"][person_id]])\n",
    "\n",
    "with open('head_pose_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Looking Forward (s)\", \"Looking Left (s)\", \"Looking Right (s)\", \"Looking Up (s)\", \"Looking Down (s)\"])\n",
    "    for person_id in time_forward_seconds:\n",
    "        writer.writerow([person_id, time_forward_seconds[person_id], time_left_seconds[person_id], time_right_seconds[person_id], time_up_seconds[person_id], time_down_seconds[person_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4c9f5-0657-4b93-a2dc-89240ebd3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Custom deserialization function for VarianceScaling initializer\n",
    "def custom_initializer(config):\n",
    "    if 'dtype' in config:\n",
    "        del config['dtype']\n",
    "    return tf.keras.initializers.VarianceScaling(**config)\n",
    "\n",
    "# Register the custom deserialization function\n",
    "tf.keras.utils.get_custom_objects().update({\n",
    "    'VarianceScaling': custom_initializer\n",
    "})\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame, shape):\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "    return left_eye, right_eye\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point[0], point[1]) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(\"brainy-bits/brainy-bits-ai/notebooks/video.h5\")\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/content/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = {}\n",
    "duration_looking_left = {}\n",
    "duration_looking_right = {}\n",
    "duration_looking_straight = {}\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = {}\n",
    "count_right = {}\n",
    "count_straight = {}\n",
    "\n",
    "# Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "emotion_duration = {\"angry\": {}, \"sad\": {}, \"happy\": {}, \"fear\": {}, \"disgust\": {}, \"neutral\": {}, \"surprise\": {}}\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = {}\n",
    "time_left_seconds = {}\n",
    "time_right_seconds = {}\n",
    "time_up_seconds = {}\n",
    "time_down_seconds = {}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        person_id = f\"Person {i+1}\"\n",
    "\n",
    "        if person_id not in duration_eyes_closed:\n",
    "            duration_eyes_closed[person_id] = 0\n",
    "            duration_looking_left[person_id] = 0\n",
    "            duration_looking_right[person_id] = 0\n",
    "            duration_looking_straight[person_id] = 0\n",
    "            count_left[person_id] = 0\n",
    "            count_right[person_id] = 0\n",
    "            count_straight[person_id] = 0\n",
    "            time_forward_seconds[person_id] = 0\n",
    "            time_left_seconds[person_id] = 0\n",
    "            time_right_seconds[person_id] = 0\n",
    "            time_up_seconds[person_id] = 0\n",
    "            time_down_seconds[person_id] = 0\n",
    "            for emotion in emotion_duration:\n",
    "                emotion_duration[emotion][person_id] = 0\n",
    "\n",
    "        # Eye tracking\n",
    "        left_eye, right_eye = detect_eyes(frame, shape)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "            # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "            # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, f\"{person_id}: Eyes Closed\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "                duration_eyes_closed[person_id] += 1 / fps  # Increment the duration\n",
    "                count_straight[person_id] += 1\n",
    "\n",
    "            else:\n",
    "                # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0][0] + right_eye[3][0]) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Left\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_left[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_left[person_id] += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Right\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_right[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_right[person_id] += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Straight\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_straight[person_id] += 1 / fps  # Increment the duration\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point[0], point[1]\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Emotion detection\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        if w > 0 and h > 0:  # Check if width and height are positive\n",
    "            face_crop = gray[y:y + h, x:x + w]\n",
    "            if face_crop.shape[0] > 0 and face_crop.shape[1] > 0:  # Check if face crop dimensions are positive\n",
    "                face_crop = zoom(face_crop, (48 / face_crop.shape[0], 48 / face_crop.shape[1]))\n",
    "                face_crop = face_crop.astype(np.float32)\n",
    "                face_crop /= float(face_crop.max())\n",
    "                face_crop = np.reshape(face_crop.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "                prediction = emotion_model.predict(face_crop)\n",
    "                prediction_result = np.argmax(prediction)\n",
    "\n",
    "                # Rectangle around the face\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                # Annotate main image with emotion label\n",
    "                emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "                emotion_label = emotion_labels[prediction_result]\n",
    "                cv2.putText(frame, f\"{person_id}: {emotion_label}\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                emotion_duration[emotion_label.lower()][person_id] += time.time() - emotion_start_time\n",
    "                emotion_start_time = time.time()\n",
    "\n",
    "        # Head pose estimation\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = frame.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                    if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "\n",
    "                face_2d = np.array(face_2d, dtype=np.float64)\n",
    "                face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "                # Camera matrix\n",
    "                focal_length = 1 * img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                       [0, focal_length, img_h / 2],\n",
    "                                       [0, 0, 1]])\n",
    "\n",
    "                # Distortion parameters\n",
    "                dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "                # Solve PnP\n",
    "                success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "                # Get rotational matrix\n",
    "                rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "                # Get angles\n",
    "                angles, mtx_r, mtx_q, qx, qy, qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "                # Get the y rotation degree\n",
    "                x_angle = angles[0] * 360\n",
    "                y_angle = angles[1] * 360\n",
    "                z_angle = angles[2] * 360\n",
    "\n",
    "                # See where the user's head tilting\n",
    "                if y_angle < -10:\n",
    "                    text = \"Looking Left\"\n",
    "                    time_left_seconds[person_id] += 1 / fps\n",
    "                elif y_angle > 10:\n",
    "                    text = \"Looking Right\"\n",
    "                    time_right_seconds[person_id] += 1 / fps\n",
    "                elif x_angle < -10:\n",
    "                    text = \"Looking Down\"\n",
    "                    time_down_seconds[person_id] += 1 / fps\n",
    "                elif x_angle > 10:\n",
    "                    text = \"Looking Up\"\n",
    "                    time_up_seconds[person_id] += 1 / fps\n",
    "                else:\n",
    "                    text = \"Looking Forward\"\n",
    "                    time_forward_seconds[person_id] += 1 / fps\n",
    "\n",
    "                # Display the text\n",
    "                cv2.putText(frame, f\"{person_id}: {text}\", (500, 50 + i * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Clear the output in Jupyter notebook\n",
    "    clear_output(wait=True)\n",
    "    display(frame)\n",
    "\n",
    "    # Exit the loop when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Write the recorded data to a CSV file\n",
    "with open('eye_tracking_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Duration Eyes Closed (s)\", \"Duration Looking Left (s)\", \"Duration Looking Right (s)\", \"Duration Looking Straight (s)\", \"Left Counts\", \"Right Counts\", \"Straight Counts\"])\n",
    "    for person_id in duration_eyes_closed:\n",
    "        writer.writerow([person_id, duration_eyes_closed[person_id], duration_looking_left[person_id], duration_looking_right[person_id], duration_looking_straight[person_id], count_left[person_id], count_right[person_id], count_straight[person_id]])\n",
    "\n",
    "with open('emotion_detection_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Angry (s)\", \"Sad (s)\", \"Happy (s)\", \"Fear (s)\", \"Disgust (s)\", \"Neutral (s)\", \"Surprise (s)\"])\n",
    "    for person_id in emotion_duration[\"angry\"]:\n",
    "        writer.writerow([person_id, emotion_duration[\"angry\"][person_id], emotion_duration[\"sad\"][person_id], emotion_duration[\"happy\"][person_id], emotion_duration[\"fear\"][person_id], emotion_duration[\"disgust\"][person_id], emotion_duration[\"neutral\"][person_id], emotion_duration[\"surprise\"][person_id]])\n",
    "\n",
    "with open('head_pose_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Looking Forward (s)\", \"Looking Left (s)\", \"Looking Right (s)\", \"Looking Up (s)\", \"Looking Down (s)\"])\n",
    "    for person_id in time_forward_seconds:\n",
    "        writer.writerow([person_id, time_forward_seconds[person_id], time_left_seconds[person_id], time_right_seconds[person_id], time_up_seconds[person_id], time_down_seconds[person_id]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ba5f5e-f9a4-4aa5-b157-57e94b3a49bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error when deserializing class 'Conv2D' using config={'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 32, 'kernel_size': [3, 3], 'strides': [2, 2], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}.\n\nException encountered: <class 'keras.src.initializers.constant_initializers.Zeros'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {'dtype': 'float32'}, 'registered_name': None}.\n\nException encountered: Zeros() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\saving\\serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 718\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\initializers\\initializer.py:81\u001b[0m, in \u001b[0;36mInitializer.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Instantiates an initializer from a configuration dictionary.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03mExample:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    An `Initializer` instance.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Zeros() takes no arguments",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\ops\\operation.py:208\u001b[0m, in \u001b[0;36mOperation.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\conv2d.py:107\u001b[0m, in \u001b[0;36mConv2D.__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     89\u001b[0m     filters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    106\u001b[0m ):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdilation_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilation_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_initializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_initializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_regularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_regularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivity_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivity_regularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:121\u001b[0m, in \u001b[0;36mBaseConv.__init__\u001b[1;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, lora_rank, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_initializer \u001b[38;5;241m=\u001b[39m initializers\u001b[38;5;241m.\u001b[39mget(kernel_initializer)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_initializer \u001b[38;5;241m=\u001b[39m \u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_initializer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_regularizer \u001b[38;5;241m=\u001b[39m regularizers\u001b[38;5;241m.\u001b[39mget(kernel_regularizer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\initializers\\__init__.py:106\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(identifier, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 106\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(identifier, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\initializers\\__init__.py:67\u001b[0m, in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a Keras initializer object via its configuration.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALL_OBJECTS_DICT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\saving\\serialization_lib.py:581\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m deserialize_keras_object(\n\u001b[0;32m    576\u001b[0m                 serialize_with_public_fn(\n\u001b[0;32m    577\u001b[0m                     module_objects[config], config, fn_module_name\n\u001b[0;32m    578\u001b[0m                 ),\n\u001b[0;32m    579\u001b[0m                 custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    580\u001b[0m             )\n\u001b[1;32m--> 581\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m            \u001b[49m\u001b[43mserialize_with_public_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_config\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PLAIN_TYPES):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\saving\\serialization_lib.py:720\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    723\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m instances (layers, models, etc.) returned by\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    724\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `get_config()` are explicitly deserialized in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms `from_config()` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    726\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    727\u001b[0m     )\n\u001b[0;32m    728\u001b[0m build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: <class 'keras.src.initializers.constant_initializers.Zeros'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {'dtype': 'float32'}, 'registered_name': None}.\n\nException encountered: Zeros() takes no arguments",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m predictor \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mshape_predictor(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Load emotion detection model\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m emotion_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/video.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Initialize video capture from the camera\u001b[39;00m\n\u001b[0;32m     48\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# 0 corresponds to the default camera (you can change it if you have multiple cameras)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\saving\\saving_api.py:183\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    177\u001b[0m         filepath,\n\u001b[0;32m    178\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    180\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    181\u001b[0m     )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:133\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    130\u001b[0m model_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(model_config)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m saving_options\u001b[38;5;241m.\u001b[39mkeras_option_scope(use_legacy_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 133\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     load_weights_from_hdf5_group(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m], model)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:85\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[0;32m     83\u001b[0m config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODULE_OBJECTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\saving\\serialization.py:495\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    490\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(\n\u001b[0;32m    491\u001b[0m     cls_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    492\u001b[0m )\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_objects\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m arg_spec\u001b[38;5;241m.\u001b[39margs:\n\u001b[1;32m--> 495\u001b[0m     deserialized_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobject_registration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGLOBAL_CUSTOM_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m object_registration\u001b[38;5;241m.\u001b[39mCustomObjectScope(custom_objects):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\model.py:517\u001b[0m, in \u001b[0;36mModel.from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_functional_config \u001b[38;5;129;01mand\u001b[39;00m revivable_as_functional:\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;66;03m# Revive Functional model\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\functional.py:517\u001b[0m, in \u001b[0;36mfunctional_from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# First, we create all layers and enqueue nodes to be processed\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_data \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 517\u001b[0m     \u001b[43mprocess_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Then we process nodes in order of layer depth.\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m# Nodes that cannot yet be processed (if the inbound node\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# does not yet exist) are re-enqueued, and the process\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;66;03m# is repeated until all nodes are processed.\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unprocessed_nodes:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\functional.py:497\u001b[0m, in \u001b[0;36mfunctional_from_config.<locals>.process_layer\u001b[1;34m(layer_data)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Instantiate layer.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m layer_data:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# Legacy format deserialization (no \"module\" key)\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;66;03m# used for H5 and SavedModel formats\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    501\u001b[0m     layer \u001b[38;5;241m=\u001b[39m serialization_lib\u001b[38;5;241m.\u001b[39mdeserialize_keras_object(\n\u001b[0;32m    502\u001b[0m         layer_data, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[0;32m    503\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:85\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[0;32m     83\u001b[0m config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODULE_OBJECTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\saving\\serialization.py:504\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m object_registration\u001b[38;5;241m.\u001b[39mCustomObjectScope(custom_objects):\n\u001b[1;32m--> 504\u001b[0m             deserialized_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# Then `cls` may be a function returning a class.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;66;03m# in this case by convention `config` holds\u001b[39;00m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;66;03m# the kwargs of the function.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m     custom_objects \u001b[38;5;241m=\u001b[39m custom_objects \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\ops\\operation.py:210\u001b[0m, in \u001b[0;36mOperation.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when deserializing class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Error when deserializing class 'Conv2D' using config={'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 32, 'kernel_size': [3, 3], 'strides': [2, 2], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}.\n\nException encountered: <class 'keras.src.initializers.constant_initializers.Zeros'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {'dtype': 'float32'}, 'registered_name': None}.\n\nException encountered: Zeros() takes no arguments"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Custom deserialization function for VarianceScaling initializer\n",
    "def custom_initializer(config):\n",
    "    if 'dtype' in config:\n",
    "        del config['dtype']\n",
    "    return tf.keras.initializers.VarianceScaling(**config)\n",
    "\n",
    "# Register the custom deserialization function\n",
    "tf.keras.utils.get_custom_objects().update({\n",
    "    'VarianceScaling': custom_initializer\n",
    "})\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame, shape):\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "    return left_eye, right_eye\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point[0], point[1]) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/video.h5\")\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/content/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = {}\n",
    "duration_looking_left = {}\n",
    "duration_looking_right = {}\n",
    "duration_looking_straight = {}\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = {}\n",
    "count_right = {}\n",
    "count_straight = {}\n",
    "\n",
    "# Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "emotion_duration = {\"angry\": {}, \"sad\": {}, \"happy\": {}, \"fear\": {}, \"disgust\": {}, \"neutral\": {}, \"surprise\": {}}\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = {}\n",
    "time_left_seconds = {}\n",
    "time_right_seconds = {}\n",
    "time_up_seconds = {}\n",
    "time_down_seconds = {}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        person_id = f\"Person {i+1}\"\n",
    "\n",
    "        if person_id not in duration_eyes_closed:\n",
    "            duration_eyes_closed[person_id] = 0\n",
    "            duration_looking_left[person_id] = 0\n",
    "            duration_looking_right[person_id] = 0\n",
    "            duration_looking_straight[person_id] = 0\n",
    "            count_left[person_id] = 0\n",
    "            count_right[person_id] = 0\n",
    "            count_straight[person_id] = 0\n",
    "            time_forward_seconds[person_id] = 0\n",
    "            time_left_seconds[person_id] = 0\n",
    "            time_right_seconds[person_id] = 0\n",
    "            time_up_seconds[person_id] = 0\n",
    "            time_down_seconds[person_id] = 0\n",
    "            for emotion in emotion_duration:\n",
    "                emotion_duration[emotion][person_id] = 0\n",
    "\n",
    "        # Eye tracking\n",
    "        left_eye, right_eye = detect_eyes(frame, shape)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "            # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "            # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, f\"{person_id}: Eyes Closed\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "                duration_eyes_closed[person_id] += 1 / fps  # Increment the duration\n",
    "                count_straight[person_id] += 1\n",
    "\n",
    "            else:\n",
    "                # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0][0] + right_eye[3][0]) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Left\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_left[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_left[person_id] += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Right\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_right[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_right[person_id] += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Straight\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_straight[person_id] += 1 / fps  # Increment the duration\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point[0], point[1]\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Emotion detection\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        if w > 0 and h > 0:  # Check if width and height are positive\n",
    "            face_crop = gray[y:y + h, x:x + w]\n",
    "            if face_crop.shape[0] > 0 and face_crop.shape[1] > 0:  # Check if face crop dimensions are positive\n",
    "                face_crop = zoom(face_crop, (48 / face_crop.shape[0], 48 / face_crop.shape[1]))\n",
    "                face_crop = face_crop.astype(np.float32)\n",
    "                face_crop /= float(face_crop.max())\n",
    "                face_crop = np.reshape(face_crop.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "                prediction = emotion_model.predict(face_crop)\n",
    "                prediction_result = np.argmax(prediction)\n",
    "\n",
    "                # Rectangle around the face\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                # Annotate main image with emotion label\n",
    "                emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "                emotion_label = emotion_labels[prediction_result]\n",
    "                cv2.putText(frame, f\"{person_id}: {emotion_label}\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                emotion_duration[emotion_label.lower()][person_id] += time.time() - emotion_start_time\n",
    "                emotion_start_time = time.time()\n",
    "\n",
    "        # Head pose estimation\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = frame.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                    if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "\n",
    "                face_2d = np.array(face_2d, dtype=np.float64)\n",
    "                face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "                # Camera matrix\n",
    "                focal_length = 1 * img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                       [0, focal_length, img_h / 2],\n",
    "                                       [0, 0, 1]])\n",
    "\n",
    "                # Distortion parameters\n",
    "                dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "                # Solve PnP\n",
    "                success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "                # Get rotational matrix\n",
    "                rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "                # Get angles\n",
    "                angles, mtx_r, mtx_q, qx, qy, qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "                # Get the y rotation degree\n",
    "                x_angle = angles[0] * 360\n",
    "                y_angle = angles[1] * 360\n",
    "                z_angle = angles[2] * 360\n",
    "\n",
    "                # See where the user's head tilting\n",
    "                if y_angle < -10:\n",
    "                    text = \"Looking Left\"\n",
    "                    time_left_seconds[person_id] += 1 / fps\n",
    "                elif y_angle > 10:\n",
    "                    text = \"Looking Right\"\n",
    "                    time_right_seconds[person_id] += 1 / fps\n",
    "                elif x_angle < -10:\n",
    "                    text = \"Looking Down\"\n",
    "                    time_down_seconds[person_id] += 1 / fps\n",
    "                elif x_angle > 10:\n",
    "                    text = \"Looking Up\"\n",
    "                    time_up_seconds[person_id] += 1 / fps\n",
    "                else:\n",
    "                    text = \"Looking Forward\"\n",
    "                    time_forward_seconds[person_id] += 1 / fps\n",
    "\n",
    "                # Display the text\n",
    "                cv2.putText(frame, f\"{person_id}: {text}\", (500, 50 + i * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Clear the output in Jupyter notebook\n",
    "    clear_output(wait=True)\n",
    "    display(frame)\n",
    "\n",
    "    # Exit the loop when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Write the recorded data to a CSV file\n",
    "with open('eye_tracking_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Duration Eyes Closed (s)\", \"Duration Looking Left (s)\", \"Duration Looking Right (s)\", \"Duration Looking Straight (s)\", \"Left Counts\", \"Right Counts\", \"Straight Counts\"])\n",
    "    for person_id in duration_eyes_closed:\n",
    "        writer.writerow([person_id, duration_eyes_closed[person_id], duration_looking_left[person_id], duration_looking_right[person_id], duration_looking_straight[person_id], count_left[person_id], count_right[person_id], count_straight[person_id]])\n",
    "\n",
    "with open('emotion_detection_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Angry (s)\", \"Sad (s)\", \"Happy (s)\", \"Fear (s)\", \"Disgust (s)\", \"Neutral (s)\", \"Surprise (s)\"])\n",
    "    for person_id in emotion_duration[\"angry\"]:\n",
    "        writer.writerow([person_id, emotion_duration[\"angry\"][person_id], emotion_duration[\"sad\"][person_id], emotion_duration[\"happy\"][person_id], emotion_duration[\"fear\"][person_id], emotion_duration[\"disgust\"][person_id], emotion_duration[\"neutral\"][person_id], emotion_duration[\"surprise\"][person_id]])\n",
    "\n",
    "with open('head_pose_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Looking Forward (s)\", \"Looking Left (s)\", \"Looking Right (s)\", \"Looking Up (s)\", \"Looking Down (s)\"])\n",
    "    for person_id in time_forward_seconds:\n",
    "        writer.writerow([person_id, time_forward_seconds[person_id], time_left_seconds[person_id], time_right_seconds[person_id], time_up_seconds[person_id], time_down_seconds[person_id]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7a538-19a3-4f60-8a33-593a45d2dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47fb5ae0-6632-4274-93d2-9dc7429d61bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error when deserializing class 'BatchNormalization' using config={'name': 'batch_normalization', 'trainable': True, 'dtype': 'float32', 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {'dtype': 'float32'}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {'dtype': 'float32'}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'axis': 3}.\n\nException encountered: <class 'keras.src.initializers.constant_initializers.Ones'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.initializers', 'class_name': 'Ones', 'config': {'dtype': 'float32'}, 'registered_name': None}.\n\nException encountered: Ones() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\saving\\serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 718\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\initializers\\initializer.py:81\u001b[0m, in \u001b[0;36mInitializer.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Instantiates an initializer from a configuration dictionary.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03mExample:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    An `Initializer` instance.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Ones() takes no arguments",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\ops\\operation.py:208\u001b[0m, in \u001b[0;36mOperation.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:158\u001b[0m, in \u001b[0;36mBatchNormalization.__init__\u001b[1;34m(self, axis, momentum, epsilon, center, scale, beta_initializer, gamma_initializer, moving_mean_initializer, moving_variance_initializer, beta_regularizer, gamma_regularizer, beta_constraint, gamma_constraint, synchronized, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_initializer \u001b[38;5;241m=\u001b[39m initializers\u001b[38;5;241m.\u001b[39mget(beta_initializer)\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma_initializer \u001b[38;5;241m=\u001b[39m \u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma_initializer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoving_mean_initializer \u001b[38;5;241m=\u001b[39m initializers\u001b[38;5;241m.\u001b[39mget(moving_mean_initializer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\initializers\\__init__.py:106\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(identifier, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 106\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(identifier, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\initializers\\__init__.py:67\u001b[0m, in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a Keras initializer object via its configuration.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALL_OBJECTS_DICT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\saving\\serialization_lib.py:581\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m deserialize_keras_object(\n\u001b[0;32m    576\u001b[0m                 serialize_with_public_fn(\n\u001b[0;32m    577\u001b[0m                     module_objects[config], config, fn_module_name\n\u001b[0;32m    578\u001b[0m                 ),\n\u001b[0;32m    579\u001b[0m                 custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    580\u001b[0m             )\n\u001b[1;32m--> 581\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m            \u001b[49m\u001b[43mserialize_with_public_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_config\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PLAIN_TYPES):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\saving\\serialization_lib.py:720\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    721\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    723\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m instances (layers, models, etc.) returned by\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    724\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `get_config()` are explicitly deserialized in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms `from_config()` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    726\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    727\u001b[0m     )\n\u001b[0;32m    728\u001b[0m build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: <class 'keras.src.initializers.constant_initializers.Ones'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.initializers', 'class_name': 'Ones', 'config': {'dtype': 'float32'}, 'registered_name': None}.\n\nException encountered: Ones() takes no arguments",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m predictor \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mshape_predictor(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Load emotion detection model\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m emotion_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/video.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Initialize video capture from the camera\u001b[39;00m\n\u001b[0;32m     54\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# 0 corresponds to the default camera (you can change it if you have multiple cameras)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\saving\\saving_api.py:183\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    177\u001b[0m         filepath,\n\u001b[0;32m    178\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    180\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    181\u001b[0m     )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:133\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    130\u001b[0m model_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(model_config)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m saving_options\u001b[38;5;241m.\u001b[39mkeras_option_scope(use_legacy_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 133\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     load_weights_from_hdf5_group(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m], model)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:85\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[0;32m     83\u001b[0m config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODULE_OBJECTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\saving\\serialization.py:495\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    490\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(\n\u001b[0;32m    491\u001b[0m     cls_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    492\u001b[0m )\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_objects\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m arg_spec\u001b[38;5;241m.\u001b[39margs:\n\u001b[1;32m--> 495\u001b[0m     deserialized_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobject_registration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGLOBAL_CUSTOM_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m object_registration\u001b[38;5;241m.\u001b[39mCustomObjectScope(custom_objects):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\model.py:517\u001b[0m, in \u001b[0;36mModel.from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_functional_config \u001b[38;5;129;01mand\u001b[39;00m revivable_as_functional:\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;66;03m# Revive Functional model\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\functional.py:517\u001b[0m, in \u001b[0;36mfunctional_from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# First, we create all layers and enqueue nodes to be processed\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_data \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 517\u001b[0m     \u001b[43mprocess_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Then we process nodes in order of layer depth.\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m# Nodes that cannot yet be processed (if the inbound node\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# does not yet exist) are re-enqueued, and the process\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;66;03m# is repeated until all nodes are processed.\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unprocessed_nodes:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\functional.py:497\u001b[0m, in \u001b[0;36mfunctional_from_config.<locals>.process_layer\u001b[1;34m(layer_data)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Instantiate layer.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m layer_data:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# Legacy format deserialization (no \"module\" key)\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;66;03m# used for H5 and SavedModel formats\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    501\u001b[0m     layer \u001b[38;5;241m=\u001b[39m serialization_lib\u001b[38;5;241m.\u001b[39mdeserialize_keras_object(\n\u001b[0;32m    502\u001b[0m         layer_data, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[0;32m    503\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:85\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[0;32m     83\u001b[0m config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODULE_OBJECTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\legacy\\saving\\serialization.py:504\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m object_registration\u001b[38;5;241m.\u001b[39mCustomObjectScope(custom_objects):\n\u001b[1;32m--> 504\u001b[0m             deserialized_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# Then `cls` may be a function returning a class.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;66;03m# in this case by convention `config` holds\u001b[39;00m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;66;03m# the kwargs of the function.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m     custom_objects \u001b[38;5;241m=\u001b[39m custom_objects \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\ops\\operation.py:210\u001b[0m, in \u001b[0;36mOperation.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when deserializing class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Error when deserializing class 'BatchNormalization' using config={'name': 'batch_normalization', 'trainable': True, 'dtype': 'float32', 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {'dtype': 'float32'}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {'dtype': 'float32'}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None, 'axis': 3}.\n\nException encountered: <class 'keras.src.initializers.constant_initializers.Ones'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.initializers', 'class_name': 'Ones', 'config': {'dtype': 'float32'}, 'registered_name': None}.\n\nException encountered: Ones() takes no arguments"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Custom deserialization functions\n",
    "def custom_variance_scaling_initializer(config):\n",
    "    if 'dtype' in config:\n",
    "        del config['dtype']\n",
    "    return tf.keras.initializers.VarianceScaling(**config)\n",
    "\n",
    "def custom_zeros_initializer(config):\n",
    "    if 'dtype' in config:\n",
    "        del config['dtype']\n",
    "    return tf.keras.initializers.Zeros()\n",
    "\n",
    "# Register the custom deserialization functions\n",
    "tf.keras.utils.get_custom_objects().update({\n",
    "    'VarianceScaling': custom_variance_scaling_initializer,\n",
    "    'Zeros': custom_zeros_initializer\n",
    "})\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame, shape):\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "    return left_eye, right_eye\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point[0], point[1]) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(\"C:/Users/nisha/Downloads/brainy-bits/brainy-bits-ai/notebooks/video.h5\")\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('/content/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = {}\n",
    "duration_looking_left = {}\n",
    "duration_looking_right = {}\n",
    "duration_looking_straight = {}\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = {}\n",
    "count_right = {}\n",
    "count_straight = {}\n",
    "\n",
    "# Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "emotion_duration = {\"angry\": {}, \"sad\": {}, \"happy\": {}, \"fear\": {}, \"disgust\": {}, \"neutral\": {}, \"surprise\": {}}\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = {}\n",
    "time_left_seconds = {}\n",
    "time_right_seconds = {}\n",
    "time_up_seconds = {}\n",
    "time_down_seconds = {}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        person_id = f\"Person {i+1}\"\n",
    "\n",
    "        if person_id not in duration_eyes_closed:\n",
    "            duration_eyes_closed[person_id] = 0\n",
    "            duration_looking_left[person_id] = 0\n",
    "            duration_looking_right[person_id] = 0\n",
    "            duration_looking_straight[person_id] = 0\n",
    "            count_left[person_id] = 0\n",
    "            count_right[person_id] = 0\n",
    "            count_straight[person_id] = 0\n",
    "            time_forward_seconds[person_id] = 0\n",
    "            time_left_seconds[person_id] = 0\n",
    "            time_right_seconds[person_id] = 0\n",
    "            time_up_seconds[person_id] = 0\n",
    "            time_down_seconds[person_id] = 0\n",
    "            for emotion in emotion_duration:\n",
    "                emotion_duration[emotion][person_id] = 0\n",
    "\n",
    "        # Eye tracking\n",
    "        left_eye, right_eye = detect_eyes(frame, shape)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "            # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "            # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, f\"{person_id}: Eyes Closed\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "                duration_eyes_closed[person_id] += 1 / fps  # Increment the duration\n",
    "                count_straight[person_id] += 1\n",
    "\n",
    "            else:\n",
    "                # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0][0] + right_eye[3][0]) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Left\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_left[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_left[person_id] += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Right\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_right[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_right[person_id] += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Straight\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_straight[person_id] += 1 / fps  # Increment the duration\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point[0], point[1]\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Emotion detection\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        if w > 0 and h > 0:  # Check if width and height are positive\n",
    "            face_crop = gray[y:y + h, x:x + w]\n",
    "            if face_crop.shape[0] > 0 and face_crop.shape[1] > 0:  # Check if face crop dimensions are positive\n",
    "                face_crop = zoom(face_crop, (48 / face_crop.shape[0], 48 / face_crop.shape[1]))\n",
    "                face_crop = face_crop.astype(np.float32)\n",
    "                face_crop /= float(face_crop.max())\n",
    "                face_crop = np.reshape(face_crop.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "                prediction = emotion_model.predict(face_crop)\n",
    "                prediction_result = np.argmax(prediction)\n",
    "\n",
    "                # Rectangle around the face\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                # Annotate main image with emotion label\n",
    "                emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "                emotion_label = emotion_labels[prediction_result]\n",
    "                cv2.putText(frame, f\"{person_id}: {emotion_label}\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                emotion_duration[emotion_label.lower()][person_id] += time.time() - emotion_start_time\n",
    "                emotion_start_time = time.time()\n",
    "\n",
    "        # Head pose estimation\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = frame.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                    if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "\n",
    "                face_2d = np.array(face_2d, dtype=np.float64)\n",
    "                face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "                # Camera matrix\n",
    "                focal_length = 1 * img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                       [0, focal_length, img_h / 2],\n",
    "                                       [0, 0, 1]])\n",
    "\n",
    "                # Distortion parameters\n",
    "                dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "                # Solve PnP\n",
    "                success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "                # Get rotational matrix\n",
    "                rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "                # Get angles\n",
    "                angles, mtx_r, mtx_q, qx, qy, qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "                # Get the y rotation degree\n",
    "                x_angle = angles[0] * 360\n",
    "                y_angle = angles[1] * 360\n",
    "                z_angle = angles[2] * 360\n",
    "\n",
    "                # See where the user's head tilting\n",
    "                if y_angle < -10:\n",
    "                    text = \"Looking Left\"\n",
    "                    time_left_seconds[person_id] += 1 / fps\n",
    "                elif y_angle > 10:\n",
    "                    text = \"Looking Right\"\n",
    "                    time_right_seconds[person_id] += 1 / fps\n",
    "                elif x_angle < -10:\n",
    "                    text = \"Looking Down\"\n",
    "                    time_down_seconds[person_id] += 1 / fps\n",
    "                elif x_angle > 10:\n",
    "                    text = \"Looking Up\"\n",
    "                    time_up_seconds[person_id] += 1 / fps\n",
    "                else:\n",
    "                    text = \"Looking Forward\"\n",
    "                    time_forward_seconds[person_id] += 1 / fps\n",
    "\n",
    "                # Display the text\n",
    "                cv2.putText(frame, f\"{person_id}: {text}\", (500, 50 + i * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Clear the output in Jupyter notebook\n",
    "    clear_output(wait=True)\n",
    "    display(frame)\n",
    "\n",
    "    # Exit the loop when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Write the recorded data to a CSV file\n",
    "with open('eye_tracking_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Duration Eyes Closed (s)\", \"Duration Looking Left (s)\", \"Duration Looking Right (s)\", \"Duration Looking Straight (s)\", \"Left Counts\", \"Right Counts\", \"Straight Counts\"])\n",
    "    for person_id in duration_eyes_closed:\n",
    "        writer.writerow([person_id, duration_eyes_closed[person_id], duration_looking_left[person_id], duration_looking_right[person_id], duration_looking_straight[person_id], count_left[person_id], count_right[person_id], count_straight[person_id]])\n",
    "\n",
    "with open('emotion_detection_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Angry (s)\", \"Sad (s)\", \"Happy (s)\", \"Fear (s)\", \"Disgust (s)\", \"Neutral (s)\", \"Surprise (s)\"])\n",
    "    for person_id in emotion_duration[\"angry\"]:\n",
    "        writer.writerow([person_id, emotion_duration[\"angry\"][person_id], emotion_duration[\"sad\"][person_id], emotion_duration[\"happy\"][person_id], emotion_duration[\"fear\"][person_id], emotion_duration[\"disgust\"][person_id], emotion_duration[\"neutral\"][person_id], emotion_duration[\"surprise\"][person_id]])\n",
    "\n",
    "with open('head_pose_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Looking Forward (s)\", \"Looking Left (s)\", \"Looking Right (s)\", \"Looking Up (s)\", \"Looking Down (s)\"])\n",
    "    for person_id in time_forward_seconds:\n",
    "        writer.writerow([person_id, time_forward_seconds[person_id], time_left_seconds[person_id], time_right_seconds[person_id], time_up_seconds[person_id], time_down_seconds[person_id]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e9d0b-40c8-4b18-a446-4553016d1f81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
