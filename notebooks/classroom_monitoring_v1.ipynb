{
 "cells": [
  {
   "cell_type": "code",
   "id": "1c7475c5-9ec9-4d3b-9e2e-74b591d59325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:35:19.613369Z",
     "start_time": "2024-07-09T19:35:19.610921Z"
    }
   },
   "source": [
    "print(\"The following cell has the emotion detection algorithm\")\n",
    "# UNIT 1, Working perfectly"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following cell has the emotion detection algorithm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:43:37.697089Z",
     "start_time": "2024-07-09T19:43:36.942710Z"
    }
   },
   "source": [
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    if faces:\n",
    "        shape = predictor(gray, faces[0])\n",
    "        left_eye = shape.parts()[36:42]\n",
    "        right_eye = shape.parts()[42:48]\n",
    "        return left_eye, right_eye\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point.x, point.y) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"../scripts/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model('../scripts/video.h5')\n",
    "\n",
    "emotion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter('../results/outputvideo.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = 0\n",
    "duration_looking_left = 0\n",
    "duration_looking_right = 0\n",
    "duration_looking_straight = 0\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = 0\n",
    "count_right = 0\n",
    "count_straight = 0\n",
    "\n",
    "# Load face detector and shape predictor for emotion detection\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "shape_predictor = dlib.shape_predictor(\"../scripts/face_landmarks.dat\")\n",
    "\n",
    "# Initialize head pose estimation\n",
    "official_start_time = time.time()\n",
    "start_time = time.time()\n",
    "end_time = 0\n",
    "\n",
    "#Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "e_start_time = time.time()\n",
    "e_end_time = 0\n",
    "angry_emotion = 0\n",
    "sad_emotion = 0\n",
    "happy_emotion = 0\n",
    "fear_emotion = 0\n",
    "disgust_emotion = 0\n",
    "neutral_emotion = 0\n",
    "surprise_emotion = 0\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = 0\n",
    "time_left_seconds = 0\n",
    "time_right_seconds = 0\n",
    "time_up_seconds = 0\n",
    "time_down_seconds = 0\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Eye tracking\n",
    "    left_eye, right_eye = detect_eyes(frame)\n",
    "\n",
    "    if left_eye is not None and right_eye is not None:\n",
    "        ear_left = calculate_ear(left_eye)\n",
    "        ear_right = calculate_ear(right_eye)\n",
    "\n",
    "        # Calculate the average EAR for both eyes\n",
    "        avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "        # Set a threshold for distraction detection (you may need to adjust this)\n",
    "        distraction_threshold = 0.2\n",
    "\n",
    "        # Check if the person is distracted\n",
    "        if avg_ear < distraction_threshold:\n",
    "            cv2.putText(frame, \"Eyes Closed\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "            duration_eyes_closed += 1 / fps  # Increment the duration\n",
    "            count_straight += 1\n",
    "\n",
    "        else:\n",
    "            # Check gaze direction\n",
    "            horizontal_ratio = (left_eye[0].x + right_eye[3].x) / 2 / width\n",
    "            if horizontal_ratio < 0.4:\n",
    "                cv2.putText(frame, \"Looking Left\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_left += 1 / fps  # Increment the duration\n",
    "                count_left += 1\n",
    "            elif horizontal_ratio > 0.6:\n",
    "                cv2.putText(frame, \"Looking Right\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_right += 1 / fps  # Increment the duration\n",
    "                count_right += 1\n",
    "            else:\n",
    "                cv2.putText(frame, \"Looking Straight\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                duration_looking_straight += 1 / fps  # Increment the duration\n",
    "\n",
    "        # Draw contours around eyes\n",
    "        for eye in [left_eye, right_eye]:\n",
    "            for point in eye:\n",
    "                x, y = point.x, point.y\n",
    "                cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "    # Emotion detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = face_detector(gray, 1)\n",
    "\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        shape = shape_predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        face = gray[y:y + h, x:x + w]\n",
    "        face = zoom(face, (48 / face.shape[0], 48 / face.shape[1]))\n",
    "        face = face.astype(np.float32)\n",
    "        face /= float(face.max())\n",
    "        face = np.reshape(face.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "        prediction = emotion_model.predict(face)\n",
    "        prediction_result = np.argmax(prediction)\n",
    "\n",
    "        # Rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Annotate main image with emotion label\n",
    "        if prediction_result == 0:\n",
    "            cv2.putText(frame, \"Angry\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            angry_emotion += time.time() - e_start_time\n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 1:\n",
    "            cv2.putText(frame, \"Disgust\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            disgust_emotion += time.time() - e_start_time\n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 2:\n",
    "            cv2.putText(frame, \"Fear\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            fear_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 3:\n",
    "            cv2.putText(frame, \"Happy\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            happy_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 4:\n",
    "            cv2.putText(frame, \"Sad\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            sad_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        elif prediction_result == 5:\n",
    "            cv2.putText(frame, \"Surprise\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            surprise_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "        else:\n",
    "            cv2.putText(frame, \"Neutral\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            neutral_emotion += time.time() - e_start_time \n",
    "            e_start_time = time.time()\n",
    "\n",
    "    # Head pose estimation\n",
    "    startTime = time.time()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #it was 1\n",
    "#     frame = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "    frame.flags.writeable = False\n",
    "    results = face_mesh.process(frame)\n",
    "    frame.flags.writeable = True\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    img_h, img_w, img_c = frame.shape\n",
    "    face_3d = []\n",
    "    face_2d = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                    if idx == 1:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])\n",
    "\n",
    "            face_2d = np.array(face_2d, dtype=np.float64)\n",
    "            face_3d = np.array(face_3d, dtype=np.float64)\n",
    "            focal_length = 1 * img_w\n",
    "\n",
    "            cam_matrix = np.array([[focal_length, 0, img_h / 2],\n",
    "                                   [0, focal_length, img_w / 2],\n",
    "                                   [0, 0, 1]])\n",
    "\n",
    "            dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "            rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "            print(f\"X Rotation: {angles[0] * 10000}\")\n",
    "            print(f\"Y Rotation: {angles[1] * 10000}\")\n",
    "\n",
    "            if angles[1] * 10000 < -100:\n",
    "                text = \"Looking Left\"\n",
    "                time_left_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[1] * 10000 > 100:\n",
    "                text = \"Looking Right\"\n",
    "                time_right_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[0] * 10000 < -100:\n",
    "                text = \"Looking Down\"\n",
    "                time_down_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            elif angles[0] * 10000 > 200:\n",
    "                text = \"Looking Up\"\n",
    "                time_up_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            else:\n",
    "                text = \"Forward\"\n",
    "                time_forward_seconds += time.time() - start_time\n",
    "                start_time = time.time()\n",
    "\n",
    "            # Display the nose direction\n",
    "            nose_3d_projection, jacobian = cv2.projectPoints(nose_3d, rot_vec, trans_vec, cam_matrix, dist_matrix)\n",
    "\n",
    "            p1 = (int(nose_2d[0]), int(nose_2d[1]))\n",
    "            p2 = (int(nose_3d_projection[0][0][0]), int(nose_3d_projection[0][0][1]))\n",
    "\n",
    "            cv2.line(frame, p1, p2, (255, 0, 0), 2)\n",
    "\n",
    "            cv2.putText(frame, text, (width - 250, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "\n",
    "    # Open the CSV file in write mode and append the angles to it\n",
    "    with open('headPoses.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the header row if the file is empty\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\"X Rotation\", \"Y Rotation\"])\n",
    "\n",
    "        # Write the angles to the CSV file\n",
    "        #writer.writerow([angles[0] * 10000, angles[1] * 10000]) #bonbon\n",
    "\n",
    "    output_video.write(frame)  # Write the frame to the output video\n",
    "\n",
    "    # Display the frame without modifying color\n",
    "    cv2.imshow('Frame', frame)\n",
    "    # Clear the previous output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object, video writer, and close all windows\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Print the durations and most observed features for emotion detection\n",
    "print(f\"\\nEmotion Detection:\")\n",
    "print(f\"Duration of Happiness: {happy_emotion} seconds\")\n",
    "print(f\"Duration of Sadness: {sad_emotion} seconds\")\n",
    "print(f\"Duration of Disgust: {disgust_emotion} seconds\")\n",
    "print(f\"Duration of Fear: {fear_emotion} seconds\")\n",
    "print(f\"Duration of Anger: {angry_emotion} seconds\")\n",
    "print(f\"Duration of Neutral: {neutral_emotion} seconds\")\n",
    "print(f\"Duration of Surprise: {surprise_emotion} seconds\")\n",
    "\n",
    "# Determine the most observed emotions movement\n",
    "max_eye_duration = max(happy_emotion, sad_emotion, disgust_emotion, fear_emotion, angry_emotion, neutral_emotion, surprise_emotion)\n",
    "if max_eye_duration == happy_emotion:\n",
    "    print(\"The most observed emotion: Happiness\")\n",
    "elif max_eye_duration == sad_emotion:\n",
    "    print(\"The most observed emotion: Sadness\")\n",
    "elif max_eye_duration == disgust_emotion:\n",
    "    print(\"The most observed emotion: Disgust\")\n",
    "elif max_eye_duration == fear_emotion:\n",
    "    print(\"The most observed emotion: Fear\")\n",
    "elif max_eye_duration == angry_emotion:\n",
    "    print(\"The most observed emotion: Anger\")\n",
    "elif max_eye_duration == surprise_emotion:\n",
    "    print(\"The most observed emotion: Surprise\")\n",
    "else:\n",
    "    print(\"The most observed emotion: Neutral\")\n",
    "\n",
    "\n",
    "# Print the durations and most observed features for eyes\n",
    "print(f\"\\nEye Movements:\")\n",
    "print(f\"Duration taken looking right: {duration_looking_right} sec\")\n",
    "print(f\"Duration taken closed eyes: {duration_eyes_closed} sec\")\n",
    "print(f\"Duration taken looking left: {duration_looking_left} sec\")\n",
    "print(f\"Duration taken looking straight: {duration_looking_straight} sec\")\n",
    "\n",
    "# Determine the most observed eye movement\n",
    "max_eye_duration = max(duration_looking_right, duration_eyes_closed, duration_looking_left, duration_looking_straight)\n",
    "if max_eye_duration == duration_looking_right:\n",
    "    print(\"The most observed eye movement: Looking Right\")\n",
    "elif max_eye_duration == duration_eyes_closed:\n",
    "    print(\"The most observed eye movement: Eyes Closed\")\n",
    "elif max_eye_duration == duration_looking_left:\n",
    "    print(\"The most observed eye movement: Looking Left\")\n",
    "else:\n",
    "    print(\"The most observed eye movement: Looking Straight\")\n",
    "\n",
    "# Print the durations and most observed features for head pose\n",
    "print(f\"\\nHead Pose Estimation:\")\n",
    "print(f\"Duration of Time Looking Forward: {time_forward_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Up: {time_up_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Left: q{time_left_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Right: {time_right_seconds} seconds\")\n",
    "print(f\"Duration of Time Looking Down: {time_down_seconds} seconds\")\n",
    "\n",
    "# Determine the most observed eye movement\n",
    "max_eye_duration = max(time_forward_seconds, time_up_seconds, time_left_seconds, time_right_seconds, time_down_seconds)\n",
    "if max_eye_duration == time_forward_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Forward\")\n",
    "elif max_eye_duration == time_up_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Upwards\")\n",
    "elif max_eye_duration == time_left_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Left\")\n",
    "elif max_eye_duration == time_right_seconds:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Right\")\n",
    "else:\n",
    "    print('\\033[93m'+\"The most observed head pose: Facing Downwards\")\n"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error when deserializing class 'Conv2D' using config={'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 32, 'kernel_size': [3, 3], 'strides': [2, 2], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}.\n\nException encountered: <class 'keras.src.initializers.random_initializers.VarianceScaling'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.initializers', 'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None, 'dtype': 'float32'}, 'registered_name': None}.\n\nException encountered: VarianceScaling.__init__() got an unexpected keyword argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/saving/serialization_lib.py:718\u001B[0m, in \u001B[0;36mdeserialize_keras_object\u001B[0;34m(config, custom_objects, safe_mode, **kwargs)\u001B[0m\n\u001B[1;32m    717\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 718\u001B[0m     instance \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_config(inner_config)\n\u001B[1;32m    719\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/initializers/initializer.py:81\u001B[0m, in \u001B[0;36mInitializer.from_config\u001B[0;34m(cls, config)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Instantiates an initializer from a configuration dictionary.\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \n\u001B[1;32m     67\u001B[0m \u001B[38;5;124;03mExample:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;124;03m    An `Initializer` instance.\u001B[39;00m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m---> 81\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig)\n",
      "\u001B[0;31mTypeError\u001B[0m: VarianceScaling.__init__() got an unexpected keyword argument 'dtype'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/ops/operation.py:208\u001B[0m, in \u001B[0;36mOperation.from_config\u001B[0;34m(cls, config)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 208\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig)\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/layers/convolutional/conv2d.py:107\u001B[0m, in \u001B[0;36mConv2D.__init__\u001B[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     88\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     89\u001B[0m     filters,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    105\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    106\u001B[0m ):\n\u001B[0;32m--> 107\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    108\u001B[0m         rank\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m    109\u001B[0m         filters\u001B[38;5;241m=\u001B[39mfilters,\n\u001B[1;32m    110\u001B[0m         kernel_size\u001B[38;5;241m=\u001B[39mkernel_size,\n\u001B[1;32m    111\u001B[0m         strides\u001B[38;5;241m=\u001B[39mstrides,\n\u001B[1;32m    112\u001B[0m         padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m    113\u001B[0m         data_format\u001B[38;5;241m=\u001B[39mdata_format,\n\u001B[1;32m    114\u001B[0m         dilation_rate\u001B[38;5;241m=\u001B[39mdilation_rate,\n\u001B[1;32m    115\u001B[0m         groups\u001B[38;5;241m=\u001B[39mgroups,\n\u001B[1;32m    116\u001B[0m         activation\u001B[38;5;241m=\u001B[39mactivation,\n\u001B[1;32m    117\u001B[0m         use_bias\u001B[38;5;241m=\u001B[39muse_bias,\n\u001B[1;32m    118\u001B[0m         kernel_initializer\u001B[38;5;241m=\u001B[39mkernel_initializer,\n\u001B[1;32m    119\u001B[0m         bias_initializer\u001B[38;5;241m=\u001B[39mbias_initializer,\n\u001B[1;32m    120\u001B[0m         kernel_regularizer\u001B[38;5;241m=\u001B[39mkernel_regularizer,\n\u001B[1;32m    121\u001B[0m         bias_regularizer\u001B[38;5;241m=\u001B[39mbias_regularizer,\n\u001B[1;32m    122\u001B[0m         activity_regularizer\u001B[38;5;241m=\u001B[39mactivity_regularizer,\n\u001B[1;32m    123\u001B[0m         kernel_constraint\u001B[38;5;241m=\u001B[39mkernel_constraint,\n\u001B[1;32m    124\u001B[0m         bias_constraint\u001B[38;5;241m=\u001B[39mbias_constraint,\n\u001B[1;32m    125\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    126\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:120\u001B[0m, in \u001B[0;36mBaseConv.__init__\u001B[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, lora_rank, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_bias \u001B[38;5;241m=\u001B[39m use_bias\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel_initializer \u001B[38;5;241m=\u001B[39m initializers\u001B[38;5;241m.\u001B[39mget(kernel_initializer)\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias_initializer \u001B[38;5;241m=\u001B[39m initializers\u001B[38;5;241m.\u001B[39mget(bias_initializer)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/initializers/__init__.py:106\u001B[0m, in \u001B[0;36mget\u001B[0;34m(identifier)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(identifier, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m--> 106\u001B[0m     obj \u001B[38;5;241m=\u001B[39m deserialize(identifier)\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(identifier, \u001B[38;5;28mstr\u001B[39m):\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/initializers/__init__.py:67\u001B[0m, in \u001B[0;36mdeserialize\u001B[0;34m(config, custom_objects)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns a Keras initializer object via its configuration.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m serialization_lib\u001B[38;5;241m.\u001B[39mdeserialize_keras_object(\n\u001B[1;32m     68\u001B[0m     config,\n\u001B[1;32m     69\u001B[0m     module_objects\u001B[38;5;241m=\u001B[39mALL_OBJECTS_DICT,\n\u001B[1;32m     70\u001B[0m     custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects,\n\u001B[1;32m     71\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/saving/serialization_lib.py:581\u001B[0m, in \u001B[0;36mdeserialize_keras_object\u001B[0;34m(config, custom_objects, safe_mode, **kwargs)\u001B[0m\n\u001B[1;32m    575\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m deserialize_keras_object(\n\u001B[1;32m    576\u001B[0m                 serialize_with_public_fn(\n\u001B[1;32m    577\u001B[0m                     module_objects[config], config, fn_module_name\n\u001B[1;32m    578\u001B[0m                 ),\n\u001B[1;32m    579\u001B[0m                 custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects,\n\u001B[1;32m    580\u001B[0m             )\n\u001B[0;32m--> 581\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m deserialize_keras_object(\n\u001B[1;32m    582\u001B[0m             serialize_with_public_class(\n\u001B[1;32m    583\u001B[0m                 module_objects[config], inner_config\u001B[38;5;241m=\u001B[39minner_config\n\u001B[1;32m    584\u001B[0m             ),\n\u001B[1;32m    585\u001B[0m             custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects,\n\u001B[1;32m    586\u001B[0m         )\n\u001B[1;32m    588\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(config, PLAIN_TYPES):\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/saving/serialization_lib.py:720\u001B[0m, in \u001B[0;36mdeserialize_keras_object\u001B[0;34m(config, custom_objects, safe_mode, **kwargs)\u001B[0m\n\u001B[1;32m    719\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 720\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    721\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m could not be deserialized properly. Please\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    722\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m ensure that components that are Python object\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    723\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m instances (layers, models, etc.) returned by\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    724\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m `get_config()` are explicitly deserialized in the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    725\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m model\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms `from_config()` method.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    726\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mconfig=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mException encountered: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    727\u001B[0m     )\n\u001B[1;32m    728\u001B[0m build_config \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuild_config\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[0;31mTypeError\u001B[0m: <class 'keras.src.initializers.random_initializers.VarianceScaling'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.initializers', 'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None, 'dtype': 'float32'}, 'registered_name': None}.\n\nException encountered: VarianceScaling.__init__() got an unexpected keyword argument 'dtype'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 40\u001B[0m\n\u001B[1;32m     37\u001B[0m predictor \u001B[38;5;241m=\u001B[39m dlib\u001B[38;5;241m.\u001B[39mshape_predictor(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../scripts/shape_predictor_68_face_landmarks.dat\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Load emotion detection model\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m emotion_model \u001B[38;5;241m=\u001B[39m load_model(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../scripts/video.h5\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     42\u001B[0m emotion_model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Initialize video capture from the camera\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/saving/saving_api.py:183\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(filepath, custom_objects, compile, safe_mode)\u001B[0m\n\u001B[1;32m    176\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m saving_lib\u001B[38;5;241m.\u001B[39mload_model(\n\u001B[1;32m    177\u001B[0m         filepath,\n\u001B[1;32m    178\u001B[0m         custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects,\n\u001B[1;32m    179\u001B[0m         \u001B[38;5;28mcompile\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcompile\u001B[39m,\n\u001B[1;32m    180\u001B[0m         safe_mode\u001B[38;5;241m=\u001B[39msafe_mode,\n\u001B[1;32m    181\u001B[0m     )\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(filepath)\u001B[38;5;241m.\u001B[39mendswith((\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.h5\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.hdf5\u001B[39m\u001B[38;5;124m\"\u001B[39m)):\n\u001B[0;32m--> 183\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m legacy_h5_format\u001B[38;5;241m.\u001B[39mload_model_from_hdf5(\n\u001B[1;32m    184\u001B[0m         filepath, custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects, \u001B[38;5;28mcompile\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcompile\u001B[39m\n\u001B[1;32m    185\u001B[0m     )\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(filepath)\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.keras\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    188\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile not found: filepath=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilepath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    189\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease ensure the file is an accessible `.keras` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    190\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzip file.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    191\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/legacy/saving/legacy_h5_format.py:133\u001B[0m, in \u001B[0;36mload_model_from_hdf5\u001B[0;34m(filepath, custom_objects, compile)\u001B[0m\n\u001B[1;32m    130\u001B[0m model_config \u001B[38;5;241m=\u001B[39m json_utils\u001B[38;5;241m.\u001B[39mdecode(model_config)\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m saving_options\u001B[38;5;241m.\u001B[39mkeras_option_scope(use_legacy_config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m--> 133\u001B[0m     model \u001B[38;5;241m=\u001B[39m saving_utils\u001B[38;5;241m.\u001B[39mmodel_from_config(\n\u001B[1;32m    134\u001B[0m         model_config, custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects\n\u001B[1;32m    135\u001B[0m     )\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;66;03m# set weights\u001B[39;00m\n\u001B[1;32m    138\u001B[0m     load_weights_from_hdf5_group(f[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_weights\u001B[39m\u001B[38;5;124m\"\u001B[39m], model)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/legacy/saving/saving_utils.py:85\u001B[0m, in \u001B[0;36mmodel_from_config\u001B[0;34m(config, custom_objects)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;66;03m# Replace keras refs with keras\u001B[39;00m\n\u001B[1;32m     83\u001B[0m config \u001B[38;5;241m=\u001B[39m _find_replace_nested_dict(config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeras.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeras.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m serialization\u001B[38;5;241m.\u001B[39mdeserialize_keras_object(\n\u001B[1;32m     86\u001B[0m     config,\n\u001B[1;32m     87\u001B[0m     module_objects\u001B[38;5;241m=\u001B[39mMODULE_OBJECTS\u001B[38;5;241m.\u001B[39mALL_OBJECTS,\n\u001B[1;32m     88\u001B[0m     custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects,\n\u001B[1;32m     89\u001B[0m     printable_module_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayer\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     90\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/legacy/saving/serialization.py:495\u001B[0m, in \u001B[0;36mdeserialize_keras_object\u001B[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001B[0m\n\u001B[1;32m    490\u001B[0m cls_config \u001B[38;5;241m=\u001B[39m _find_replace_nested_dict(\n\u001B[1;32m    491\u001B[0m     cls_config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeras.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeras.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    492\u001B[0m )\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustom_objects\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m arg_spec\u001B[38;5;241m.\u001B[39margs:\n\u001B[0;32m--> 495\u001B[0m     deserialized_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_config(\n\u001B[1;32m    496\u001B[0m         cls_config,\n\u001B[1;32m    497\u001B[0m         custom_objects\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m    498\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mobject_registration\u001B[38;5;241m.\u001B[39mGLOBAL_CUSTOM_OBJECTS,\n\u001B[1;32m    499\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcustom_objects,\n\u001B[1;32m    500\u001B[0m         },\n\u001B[1;32m    501\u001B[0m     )\n\u001B[1;32m    502\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    503\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m object_registration\u001B[38;5;241m.\u001B[39mCustomObjectScope(custom_objects):\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/models/model.py:517\u001B[0m, in \u001B[0;36mModel.from_config\u001B[0;34m(cls, config, custom_objects)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_functional_config \u001B[38;5;129;01mand\u001B[39;00m revivable_as_functional:\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;66;03m# Revive Functional model\u001B[39;00m\n\u001B[1;32m    514\u001B[0m     \u001B[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001B[39;00m\n\u001B[1;32m    515\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functional_from_config\n\u001B[0;32m--> 517\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m functional_from_config(\n\u001B[1;32m    518\u001B[0m         \u001B[38;5;28mcls\u001B[39m, config, custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects\n\u001B[1;32m    519\u001B[0m     )\n\u001B[1;32m    521\u001B[0m \u001B[38;5;66;03m# Either the model has a custom __init__, or the config\u001B[39;00m\n\u001B[1;32m    522\u001B[0m \u001B[38;5;66;03m# does not contain all the information necessary to\u001B[39;00m\n\u001B[1;32m    523\u001B[0m \u001B[38;5;66;03m# revive a Functional model. This happens when the user creates\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    526\u001B[0m \u001B[38;5;66;03m# In this case, we fall back to provide all config into the\u001B[39;00m\n\u001B[1;32m    527\u001B[0m \u001B[38;5;66;03m# constructor of the class.\u001B[39;00m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/models/functional.py:517\u001B[0m, in \u001B[0;36mfunctional_from_config\u001B[0;34m(cls, config, custom_objects)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;66;03m# First, we create all layers and enqueue nodes to be processed\u001B[39;00m\n\u001B[1;32m    516\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer_data \u001B[38;5;129;01min\u001B[39;00m config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayers\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 517\u001B[0m     process_layer(layer_data)\n\u001B[1;32m    519\u001B[0m \u001B[38;5;66;03m# Then we process nodes in order of layer depth.\u001B[39;00m\n\u001B[1;32m    520\u001B[0m \u001B[38;5;66;03m# Nodes that cannot yet be processed (if the inbound node\u001B[39;00m\n\u001B[1;32m    521\u001B[0m \u001B[38;5;66;03m# does not yet exist) are re-enqueued, and the process\u001B[39;00m\n\u001B[1;32m    522\u001B[0m \u001B[38;5;66;03m# is repeated until all nodes are processed.\u001B[39;00m\n\u001B[1;32m    523\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m unprocessed_nodes:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/models/functional.py:497\u001B[0m, in \u001B[0;36mfunctional_from_config.<locals>.process_layer\u001B[0;34m(layer_data)\u001B[0m\n\u001B[1;32m    493\u001B[0m \u001B[38;5;66;03m# Instantiate layer.\u001B[39;00m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodule\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m layer_data:\n\u001B[1;32m    495\u001B[0m     \u001B[38;5;66;03m# Legacy format deserialization (no \"module\" key)\u001B[39;00m\n\u001B[1;32m    496\u001B[0m     \u001B[38;5;66;03m# used for H5 and SavedModel formats\u001B[39;00m\n\u001B[0;32m--> 497\u001B[0m     layer \u001B[38;5;241m=\u001B[39m saving_utils\u001B[38;5;241m.\u001B[39mmodel_from_config(\n\u001B[1;32m    498\u001B[0m         layer_data, custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects\n\u001B[1;32m    499\u001B[0m     )\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    501\u001B[0m     layer \u001B[38;5;241m=\u001B[39m serialization_lib\u001B[38;5;241m.\u001B[39mdeserialize_keras_object(\n\u001B[1;32m    502\u001B[0m         layer_data, custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects\n\u001B[1;32m    503\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/legacy/saving/saving_utils.py:85\u001B[0m, in \u001B[0;36mmodel_from_config\u001B[0;34m(config, custom_objects)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;66;03m# Replace keras refs with keras\u001B[39;00m\n\u001B[1;32m     83\u001B[0m config \u001B[38;5;241m=\u001B[39m _find_replace_nested_dict(config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeras.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeras.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m serialization\u001B[38;5;241m.\u001B[39mdeserialize_keras_object(\n\u001B[1;32m     86\u001B[0m     config,\n\u001B[1;32m     87\u001B[0m     module_objects\u001B[38;5;241m=\u001B[39mMODULE_OBJECTS\u001B[38;5;241m.\u001B[39mALL_OBJECTS,\n\u001B[1;32m     88\u001B[0m     custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects,\n\u001B[1;32m     89\u001B[0m     printable_module_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayer\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     90\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/legacy/saving/serialization.py:504\u001B[0m, in \u001B[0;36mdeserialize_keras_object\u001B[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001B[0m\n\u001B[1;32m    502\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    503\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m object_registration\u001B[38;5;241m.\u001B[39mCustomObjectScope(custom_objects):\n\u001B[0;32m--> 504\u001B[0m             deserialized_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mfrom_config(cls_config)\n\u001B[1;32m    505\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    506\u001B[0m     \u001B[38;5;66;03m# Then `cls` may be a function returning a class.\u001B[39;00m\n\u001B[1;32m    507\u001B[0m     \u001B[38;5;66;03m# in this case by convention `config` holds\u001B[39;00m\n\u001B[1;32m    508\u001B[0m     \u001B[38;5;66;03m# the kwargs of the function.\u001B[39;00m\n\u001B[1;32m    509\u001B[0m     custom_objects \u001B[38;5;241m=\u001B[39m custom_objects \u001B[38;5;129;01mor\u001B[39;00m {}\n",
      "File \u001B[0;32m/opt/anaconda3/envs/brainy-bits/lib/python3.12/site-packages/keras/src/ops/operation.py:210\u001B[0m, in \u001B[0;36mOperation.from_config\u001B[0;34m(cls, config)\u001B[0m\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig)\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 210\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    211\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError when deserializing class \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m using \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    212\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mException encountered: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    213\u001B[0m     )\n",
      "\u001B[0;31mTypeError\u001B[0m: Error when deserializing class 'Conv2D' using config={'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 32, 'kernel_size': [3, 3], 'strides': [2, 2], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}.\n\nException encountered: <class 'keras.src.initializers.random_initializers.VarianceScaling'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.initializers', 'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None, 'dtype': 'float32'}, 'registered_name': None}.\n\nException encountered: VarianceScaling.__init__() got an unexpected keyword argument 'dtype'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "8812bfdb7783986f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:53:35.698386Z",
     "start_time": "2024-07-09T19:53:35.695629Z"
    }
   },
   "source": [
    "print(\"The following cell has multiple face detection algorithm\")\n",
    "# UNIT 2, Working perfectly"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following cell has multiple face detection algorithm\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "cc2fd3b33d18729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:54:20.130056Z",
     "start_time": "2024-07-09T19:53:39.601278Z"
    }
   },
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Initialize the video capture (use 0 for the first camera device)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize dlib's face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "person_count = 1\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the grayscale frame\n",
    "    faces = detector(gray)\n",
    "\n",
    "    # Loop through each detected face\n",
    "    for i, face in enumerate(faces):\n",
    "        # Draw a rectangle around the face\n",
    "        x, y, w, h = (face.left(), face.top(), face.width(), face.height())\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # Label the face with a unique identifier\n",
    "        label = f'Person {i+1}'\n",
    "        cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Face Detection', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 15:53:39.792 python[7221:410190] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 22\u001B[0m\n\u001B[1;32m     19\u001B[0m gray \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(frame, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2GRAY)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Detect faces in the grayscale frame\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m faces \u001B[38;5;241m=\u001B[39m detector(gray)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Loop through each detected face\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, face \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(faces):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;66;03m# Draw a rectangle around the face\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "07f859b5-7510-4e5c-b858-c2394020996a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:54:41.515343Z",
     "start_time": "2024-07-09T19:54:41.512126Z"
    }
   },
   "source": [
    "# Test code 1\n",
    "print(\"Simple code to read the .h5 file\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple code to read the .h5 file\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ab20e00f-ded7-4ead-8813-7a3413f01d51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:55:04.873281Z",
     "start_time": "2024-07-09T19:55:04.726567Z"
    }
   },
   "source": [
    "import h5py\n",
    "\n",
    "# Path to the HDF5 file\n",
    "h5_file_path = '../scripts/video_modified.h5'\n",
    "\n",
    "# Open the HDF5 file for reading\n",
    "with h5py.File(h5_file_path, 'r') as f:\n",
    "    print(\"Keys in the HDF5 file:\")\n",
    "    print(\"======================\")\n",
    "    # Print all groups and datasets\n",
    "    def print_attrs(name, obj):\n",
    "        print(name)\n",
    "        for key, val in obj.attrs.items():\n",
    "            print(\"    %s: %s\" % (key, val))\n",
    "\n",
    "    f.visititems(print_attrs)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the HDF5 file:\n",
      "======================\n",
      "model_weights\n",
      "    backend: tensorflow\n",
      "    keras_version: 2.1.6-tf\n",
      "    layer_names: [b'input_1' b'conv2d' b'batch_normalization' b'activation' b'conv2d_1'\n",
      " b'batch_normalization_1' b'activation_1' b'activation_2'\n",
      " b'separable_conv2d' b'batch_normalization_2' b'activation_3'\n",
      " b'separable_conv2d_1' b'batch_normalization_3' b'max_pooling2d'\n",
      " b'conv2d_2' b'add' b'activation_4' b'separable_conv2d_2'\n",
      " b'batch_normalization_4' b'activation_5' b'separable_conv2d_3'\n",
      " b'batch_normalization_5' b'max_pooling2d_1' b'conv2d_3' b'add_1'\n",
      " b'activation_6' b'separable_conv2d_4' b'batch_normalization_6'\n",
      " b'activation_7' b'separable_conv2d_5' b'batch_normalization_7'\n",
      " b'max_pooling2d_2' b'conv2d_4' b'add_2' b'activation_8'\n",
      " b'separable_conv2d_6' b'batch_normalization_8' b'activation_9'\n",
      " b'separable_conv2d_7' b'batch_normalization_9' b'activation_10'\n",
      " b'separable_conv2d_8' b'batch_normalization_10' b'add_3' b'activation_11'\n",
      " b'separable_conv2d_9' b'batch_normalization_11' b'activation_12'\n",
      " b'separable_conv2d_10' b'batch_normalization_12' b'activation_13'\n",
      " b'separable_conv2d_11' b'batch_normalization_13' b'add_4'\n",
      " b'activation_14' b'separable_conv2d_12' b'batch_normalization_14'\n",
      " b'activation_15' b'separable_conv2d_13' b'batch_normalization_15'\n",
      " b'activation_16' b'separable_conv2d_14' b'batch_normalization_16'\n",
      " b'add_5' b'activation_17' b'separable_conv2d_15'\n",
      " b'batch_normalization_17' b'activation_18' b'separable_conv2d_16'\n",
      " b'batch_normalization_18' b'activation_19' b'separable_conv2d_17'\n",
      " b'batch_normalization_19' b'add_6' b'activation_20'\n",
      " b'separable_conv2d_18' b'batch_normalization_20' b'activation_21'\n",
      " b'separable_conv2d_19' b'batch_normalization_21' b'activation_22'\n",
      " b'separable_conv2d_20' b'batch_normalization_22' b'add_7'\n",
      " b'activation_23' b'separable_conv2d_21' b'batch_normalization_23'\n",
      " b'activation_24' b'separable_conv2d_22' b'batch_normalization_24'\n",
      " b'activation_25' b'separable_conv2d_23' b'batch_normalization_25'\n",
      " b'add_8' b'activation_26' b'separable_conv2d_24'\n",
      " b'batch_normalization_26' b'activation_27' b'separable_conv2d_25'\n",
      " b'batch_normalization_27' b'activation_28' b'separable_conv2d_26'\n",
      " b'batch_normalization_28' b'add_9' b'activation_29'\n",
      " b'separable_conv2d_27' b'batch_normalization_29' b'activation_30'\n",
      " b'separable_conv2d_28' b'batch_normalization_30' b'activation_31'\n",
      " b'separable_conv2d_29' b'batch_normalization_31' b'add_10'\n",
      " b'activation_32' b'separable_conv2d_30' b'batch_normalization_32'\n",
      " b'activation_33' b'separable_conv2d_31' b'batch_normalization_33'\n",
      " b'max_pooling2d_3' b'conv2d_5' b'add_11' b'activation_34'\n",
      " b'separable_conv2d_32' b'batch_normalization_34' b'activation_35'\n",
      " b'separable_conv2d_33' b'batch_normalization_35'\n",
      " b'global_average_pooling2d' b'dense']\n",
      "model_weights/activation\n",
      "    weight_names: []\n",
      "model_weights/activation_1\n",
      "    weight_names: []\n",
      "model_weights/activation_10\n",
      "    weight_names: []\n",
      "model_weights/activation_11\n",
      "    weight_names: []\n",
      "model_weights/activation_12\n",
      "    weight_names: []\n",
      "model_weights/activation_13\n",
      "    weight_names: []\n",
      "model_weights/activation_14\n",
      "    weight_names: []\n",
      "model_weights/activation_15\n",
      "    weight_names: []\n",
      "model_weights/activation_16\n",
      "    weight_names: []\n",
      "model_weights/activation_17\n",
      "    weight_names: []\n",
      "model_weights/activation_18\n",
      "    weight_names: []\n",
      "model_weights/activation_19\n",
      "    weight_names: []\n",
      "model_weights/activation_2\n",
      "    weight_names: []\n",
      "model_weights/activation_20\n",
      "    weight_names: []\n",
      "model_weights/activation_21\n",
      "    weight_names: []\n",
      "model_weights/activation_22\n",
      "    weight_names: []\n",
      "model_weights/activation_23\n",
      "    weight_names: []\n",
      "model_weights/activation_24\n",
      "    weight_names: []\n",
      "model_weights/activation_25\n",
      "    weight_names: []\n",
      "model_weights/activation_26\n",
      "    weight_names: []\n",
      "model_weights/activation_27\n",
      "    weight_names: []\n",
      "model_weights/activation_28\n",
      "    weight_names: []\n",
      "model_weights/activation_29\n",
      "    weight_names: []\n",
      "model_weights/activation_3\n",
      "    weight_names: []\n",
      "model_weights/activation_30\n",
      "    weight_names: []\n",
      "model_weights/activation_31\n",
      "    weight_names: []\n",
      "model_weights/activation_32\n",
      "    weight_names: []\n",
      "model_weights/activation_33\n",
      "    weight_names: []\n",
      "model_weights/activation_34\n",
      "    weight_names: []\n",
      "model_weights/activation_35\n",
      "    weight_names: []\n",
      "model_weights/activation_4\n",
      "    weight_names: []\n",
      "model_weights/activation_5\n",
      "    weight_names: []\n",
      "model_weights/activation_6\n",
      "    weight_names: []\n",
      "model_weights/activation_7\n",
      "    weight_names: []\n",
      "model_weights/activation_8\n",
      "    weight_names: []\n",
      "model_weights/activation_9\n",
      "    weight_names: []\n",
      "model_weights/add\n",
      "    weight_names: []\n",
      "model_weights/add_1\n",
      "    weight_names: []\n",
      "model_weights/add_10\n",
      "    weight_names: []\n",
      "model_weights/add_11\n",
      "    weight_names: []\n",
      "model_weights/add_2\n",
      "    weight_names: []\n",
      "model_weights/add_3\n",
      "    weight_names: []\n",
      "model_weights/add_4\n",
      "    weight_names: []\n",
      "model_weights/add_5\n",
      "    weight_names: []\n",
      "model_weights/add_6\n",
      "    weight_names: []\n",
      "model_weights/add_7\n",
      "    weight_names: []\n",
      "model_weights/add_8\n",
      "    weight_names: []\n",
      "model_weights/add_9\n",
      "    weight_names: []\n",
      "model_weights/batch_normalization\n",
      "    weight_names: [b'batch_normalization/gamma:0' b'batch_normalization/beta:0'\n",
      " b'batch_normalization/moving_mean:0'\n",
      " b'batch_normalization/moving_variance:0']\n",
      "model_weights/batch_normalization/batch_normalization\n",
      "model_weights/batch_normalization/batch_normalization/beta:0\n",
      "model_weights/batch_normalization/batch_normalization/gamma:0\n",
      "model_weights/batch_normalization/batch_normalization/moving_mean:0\n",
      "model_weights/batch_normalization/batch_normalization/moving_variance:0\n",
      "model_weights/batch_normalization_1\n",
      "    weight_names: [b'batch_normalization_1/gamma:0' b'batch_normalization_1/beta:0'\n",
      " b'batch_normalization_1/moving_mean:0'\n",
      " b'batch_normalization_1/moving_variance:0']\n",
      "model_weights/batch_normalization_1/batch_normalization_1\n",
      "model_weights/batch_normalization_1/batch_normalization_1/beta:0\n",
      "model_weights/batch_normalization_1/batch_normalization_1/gamma:0\n",
      "model_weights/batch_normalization_1/batch_normalization_1/moving_mean:0\n",
      "model_weights/batch_normalization_1/batch_normalization_1/moving_variance:0\n",
      "model_weights/batch_normalization_10\n",
      "    weight_names: [b'batch_normalization_10/gamma:0' b'batch_normalization_10/beta:0'\n",
      " b'batch_normalization_10/moving_mean:0'\n",
      " b'batch_normalization_10/moving_variance:0']\n",
      "model_weights/batch_normalization_10/batch_normalization_10\n",
      "model_weights/batch_normalization_10/batch_normalization_10/beta:0\n",
      "model_weights/batch_normalization_10/batch_normalization_10/gamma:0\n",
      "model_weights/batch_normalization_10/batch_normalization_10/moving_mean:0\n",
      "model_weights/batch_normalization_10/batch_normalization_10/moving_variance:0\n",
      "model_weights/batch_normalization_11\n",
      "    weight_names: [b'batch_normalization_11/gamma:0' b'batch_normalization_11/beta:0'\n",
      " b'batch_normalization_11/moving_mean:0'\n",
      " b'batch_normalization_11/moving_variance:0']\n",
      "model_weights/batch_normalization_11/batch_normalization_11\n",
      "model_weights/batch_normalization_11/batch_normalization_11/beta:0\n",
      "model_weights/batch_normalization_11/batch_normalization_11/gamma:0\n",
      "model_weights/batch_normalization_11/batch_normalization_11/moving_mean:0\n",
      "model_weights/batch_normalization_11/batch_normalization_11/moving_variance:0\n",
      "model_weights/batch_normalization_12\n",
      "    weight_names: [b'batch_normalization_12/gamma:0' b'batch_normalization_12/beta:0'\n",
      " b'batch_normalization_12/moving_mean:0'\n",
      " b'batch_normalization_12/moving_variance:0']\n",
      "model_weights/batch_normalization_12/batch_normalization_12\n",
      "model_weights/batch_normalization_12/batch_normalization_12/beta:0\n",
      "model_weights/batch_normalization_12/batch_normalization_12/gamma:0\n",
      "model_weights/batch_normalization_12/batch_normalization_12/moving_mean:0\n",
      "model_weights/batch_normalization_12/batch_normalization_12/moving_variance:0\n",
      "model_weights/batch_normalization_13\n",
      "    weight_names: [b'batch_normalization_13/gamma:0' b'batch_normalization_13/beta:0'\n",
      " b'batch_normalization_13/moving_mean:0'\n",
      " b'batch_normalization_13/moving_variance:0']\n",
      "model_weights/batch_normalization_13/batch_normalization_13\n",
      "model_weights/batch_normalization_13/batch_normalization_13/beta:0\n",
      "model_weights/batch_normalization_13/batch_normalization_13/gamma:0\n",
      "model_weights/batch_normalization_13/batch_normalization_13/moving_mean:0\n",
      "model_weights/batch_normalization_13/batch_normalization_13/moving_variance:0\n",
      "model_weights/batch_normalization_14\n",
      "    weight_names: [b'batch_normalization_14/gamma:0' b'batch_normalization_14/beta:0'\n",
      " b'batch_normalization_14/moving_mean:0'\n",
      " b'batch_normalization_14/moving_variance:0']\n",
      "model_weights/batch_normalization_14/batch_normalization_14\n",
      "model_weights/batch_normalization_14/batch_normalization_14/beta:0\n",
      "model_weights/batch_normalization_14/batch_normalization_14/gamma:0\n",
      "model_weights/batch_normalization_14/batch_normalization_14/moving_mean:0\n",
      "model_weights/batch_normalization_14/batch_normalization_14/moving_variance:0\n",
      "model_weights/batch_normalization_15\n",
      "    weight_names: [b'batch_normalization_15/gamma:0' b'batch_normalization_15/beta:0'\n",
      " b'batch_normalization_15/moving_mean:0'\n",
      " b'batch_normalization_15/moving_variance:0']\n",
      "model_weights/batch_normalization_15/batch_normalization_15\n",
      "model_weights/batch_normalization_15/batch_normalization_15/beta:0\n",
      "model_weights/batch_normalization_15/batch_normalization_15/gamma:0\n",
      "model_weights/batch_normalization_15/batch_normalization_15/moving_mean:0\n",
      "model_weights/batch_normalization_15/batch_normalization_15/moving_variance:0\n",
      "model_weights/batch_normalization_16\n",
      "    weight_names: [b'batch_normalization_16/gamma:0' b'batch_normalization_16/beta:0'\n",
      " b'batch_normalization_16/moving_mean:0'\n",
      " b'batch_normalization_16/moving_variance:0']\n",
      "model_weights/batch_normalization_16/batch_normalization_16\n",
      "model_weights/batch_normalization_16/batch_normalization_16/beta:0\n",
      "model_weights/batch_normalization_16/batch_normalization_16/gamma:0\n",
      "model_weights/batch_normalization_16/batch_normalization_16/moving_mean:0\n",
      "model_weights/batch_normalization_16/batch_normalization_16/moving_variance:0\n",
      "model_weights/batch_normalization_17\n",
      "    weight_names: [b'batch_normalization_17/gamma:0' b'batch_normalization_17/beta:0'\n",
      " b'batch_normalization_17/moving_mean:0'\n",
      " b'batch_normalization_17/moving_variance:0']\n",
      "model_weights/batch_normalization_17/batch_normalization_17\n",
      "model_weights/batch_normalization_17/batch_normalization_17/beta:0\n",
      "model_weights/batch_normalization_17/batch_normalization_17/gamma:0\n",
      "model_weights/batch_normalization_17/batch_normalization_17/moving_mean:0\n",
      "model_weights/batch_normalization_17/batch_normalization_17/moving_variance:0\n",
      "model_weights/batch_normalization_18\n",
      "    weight_names: [b'batch_normalization_18/gamma:0' b'batch_normalization_18/beta:0'\n",
      " b'batch_normalization_18/moving_mean:0'\n",
      " b'batch_normalization_18/moving_variance:0']\n",
      "model_weights/batch_normalization_18/batch_normalization_18\n",
      "model_weights/batch_normalization_18/batch_normalization_18/beta:0\n",
      "model_weights/batch_normalization_18/batch_normalization_18/gamma:0\n",
      "model_weights/batch_normalization_18/batch_normalization_18/moving_mean:0\n",
      "model_weights/batch_normalization_18/batch_normalization_18/moving_variance:0\n",
      "model_weights/batch_normalization_19\n",
      "    weight_names: [b'batch_normalization_19/gamma:0' b'batch_normalization_19/beta:0'\n",
      " b'batch_normalization_19/moving_mean:0'\n",
      " b'batch_normalization_19/moving_variance:0']\n",
      "model_weights/batch_normalization_19/batch_normalization_19\n",
      "model_weights/batch_normalization_19/batch_normalization_19/beta:0\n",
      "model_weights/batch_normalization_19/batch_normalization_19/gamma:0\n",
      "model_weights/batch_normalization_19/batch_normalization_19/moving_mean:0\n",
      "model_weights/batch_normalization_19/batch_normalization_19/moving_variance:0\n",
      "model_weights/batch_normalization_2\n",
      "    weight_names: [b'batch_normalization_2/gamma:0' b'batch_normalization_2/beta:0'\n",
      " b'batch_normalization_2/moving_mean:0'\n",
      " b'batch_normalization_2/moving_variance:0']\n",
      "model_weights/batch_normalization_2/batch_normalization_2\n",
      "model_weights/batch_normalization_2/batch_normalization_2/beta:0\n",
      "model_weights/batch_normalization_2/batch_normalization_2/gamma:0\n",
      "model_weights/batch_normalization_2/batch_normalization_2/moving_mean:0\n",
      "model_weights/batch_normalization_2/batch_normalization_2/moving_variance:0\n",
      "model_weights/batch_normalization_20\n",
      "    weight_names: [b'batch_normalization_20/gamma:0' b'batch_normalization_20/beta:0'\n",
      " b'batch_normalization_20/moving_mean:0'\n",
      " b'batch_normalization_20/moving_variance:0']\n",
      "model_weights/batch_normalization_20/batch_normalization_20\n",
      "model_weights/batch_normalization_20/batch_normalization_20/beta:0\n",
      "model_weights/batch_normalization_20/batch_normalization_20/gamma:0\n",
      "model_weights/batch_normalization_20/batch_normalization_20/moving_mean:0\n",
      "model_weights/batch_normalization_20/batch_normalization_20/moving_variance:0\n",
      "model_weights/batch_normalization_21\n",
      "    weight_names: [b'batch_normalization_21/gamma:0' b'batch_normalization_21/beta:0'\n",
      " b'batch_normalization_21/moving_mean:0'\n",
      " b'batch_normalization_21/moving_variance:0']\n",
      "model_weights/batch_normalization_21/batch_normalization_21\n",
      "model_weights/batch_normalization_21/batch_normalization_21/beta:0\n",
      "model_weights/batch_normalization_21/batch_normalization_21/gamma:0\n",
      "model_weights/batch_normalization_21/batch_normalization_21/moving_mean:0\n",
      "model_weights/batch_normalization_21/batch_normalization_21/moving_variance:0\n",
      "model_weights/batch_normalization_22\n",
      "    weight_names: [b'batch_normalization_22/gamma:0' b'batch_normalization_22/beta:0'\n",
      " b'batch_normalization_22/moving_mean:0'\n",
      " b'batch_normalization_22/moving_variance:0']\n",
      "model_weights/batch_normalization_22/batch_normalization_22\n",
      "model_weights/batch_normalization_22/batch_normalization_22/beta:0\n",
      "model_weights/batch_normalization_22/batch_normalization_22/gamma:0\n",
      "model_weights/batch_normalization_22/batch_normalization_22/moving_mean:0\n",
      "model_weights/batch_normalization_22/batch_normalization_22/moving_variance:0\n",
      "model_weights/batch_normalization_23\n",
      "    weight_names: [b'batch_normalization_23/gamma:0' b'batch_normalization_23/beta:0'\n",
      " b'batch_normalization_23/moving_mean:0'\n",
      " b'batch_normalization_23/moving_variance:0']\n",
      "model_weights/batch_normalization_23/batch_normalization_23\n",
      "model_weights/batch_normalization_23/batch_normalization_23/beta:0\n",
      "model_weights/batch_normalization_23/batch_normalization_23/gamma:0\n",
      "model_weights/batch_normalization_23/batch_normalization_23/moving_mean:0\n",
      "model_weights/batch_normalization_23/batch_normalization_23/moving_variance:0\n",
      "model_weights/batch_normalization_24\n",
      "    weight_names: [b'batch_normalization_24/gamma:0' b'batch_normalization_24/beta:0'\n",
      " b'batch_normalization_24/moving_mean:0'\n",
      " b'batch_normalization_24/moving_variance:0']\n",
      "model_weights/batch_normalization_24/batch_normalization_24\n",
      "model_weights/batch_normalization_24/batch_normalization_24/beta:0\n",
      "model_weights/batch_normalization_24/batch_normalization_24/gamma:0\n",
      "model_weights/batch_normalization_24/batch_normalization_24/moving_mean:0\n",
      "model_weights/batch_normalization_24/batch_normalization_24/moving_variance:0\n",
      "model_weights/batch_normalization_25\n",
      "    weight_names: [b'batch_normalization_25/gamma:0' b'batch_normalization_25/beta:0'\n",
      " b'batch_normalization_25/moving_mean:0'\n",
      " b'batch_normalization_25/moving_variance:0']\n",
      "model_weights/batch_normalization_25/batch_normalization_25\n",
      "model_weights/batch_normalization_25/batch_normalization_25/beta:0\n",
      "model_weights/batch_normalization_25/batch_normalization_25/gamma:0\n",
      "model_weights/batch_normalization_25/batch_normalization_25/moving_mean:0\n",
      "model_weights/batch_normalization_25/batch_normalization_25/moving_variance:0\n",
      "model_weights/batch_normalization_26\n",
      "    weight_names: [b'batch_normalization_26/gamma:0' b'batch_normalization_26/beta:0'\n",
      " b'batch_normalization_26/moving_mean:0'\n",
      " b'batch_normalization_26/moving_variance:0']\n",
      "model_weights/batch_normalization_26/batch_normalization_26\n",
      "model_weights/batch_normalization_26/batch_normalization_26/beta:0\n",
      "model_weights/batch_normalization_26/batch_normalization_26/gamma:0\n",
      "model_weights/batch_normalization_26/batch_normalization_26/moving_mean:0\n",
      "model_weights/batch_normalization_26/batch_normalization_26/moving_variance:0\n",
      "model_weights/batch_normalization_27\n",
      "    weight_names: [b'batch_normalization_27/gamma:0' b'batch_normalization_27/beta:0'\n",
      " b'batch_normalization_27/moving_mean:0'\n",
      " b'batch_normalization_27/moving_variance:0']\n",
      "model_weights/batch_normalization_27/batch_normalization_27\n",
      "model_weights/batch_normalization_27/batch_normalization_27/beta:0\n",
      "model_weights/batch_normalization_27/batch_normalization_27/gamma:0\n",
      "model_weights/batch_normalization_27/batch_normalization_27/moving_mean:0\n",
      "model_weights/batch_normalization_27/batch_normalization_27/moving_variance:0\n",
      "model_weights/batch_normalization_28\n",
      "    weight_names: [b'batch_normalization_28/gamma:0' b'batch_normalization_28/beta:0'\n",
      " b'batch_normalization_28/moving_mean:0'\n",
      " b'batch_normalization_28/moving_variance:0']\n",
      "model_weights/batch_normalization_28/batch_normalization_28\n",
      "model_weights/batch_normalization_28/batch_normalization_28/beta:0\n",
      "model_weights/batch_normalization_28/batch_normalization_28/gamma:0\n",
      "model_weights/batch_normalization_28/batch_normalization_28/moving_mean:0\n",
      "model_weights/batch_normalization_28/batch_normalization_28/moving_variance:0\n",
      "model_weights/batch_normalization_29\n",
      "    weight_names: [b'batch_normalization_29/gamma:0' b'batch_normalization_29/beta:0'\n",
      " b'batch_normalization_29/moving_mean:0'\n",
      " b'batch_normalization_29/moving_variance:0']\n",
      "model_weights/batch_normalization_29/batch_normalization_29\n",
      "model_weights/batch_normalization_29/batch_normalization_29/beta:0\n",
      "model_weights/batch_normalization_29/batch_normalization_29/gamma:0\n",
      "model_weights/batch_normalization_29/batch_normalization_29/moving_mean:0\n",
      "model_weights/batch_normalization_29/batch_normalization_29/moving_variance:0\n",
      "model_weights/batch_normalization_3\n",
      "    weight_names: [b'batch_normalization_3/gamma:0' b'batch_normalization_3/beta:0'\n",
      " b'batch_normalization_3/moving_mean:0'\n",
      " b'batch_normalization_3/moving_variance:0']\n",
      "model_weights/batch_normalization_3/batch_normalization_3\n",
      "model_weights/batch_normalization_3/batch_normalization_3/beta:0\n",
      "model_weights/batch_normalization_3/batch_normalization_3/gamma:0\n",
      "model_weights/batch_normalization_3/batch_normalization_3/moving_mean:0\n",
      "model_weights/batch_normalization_3/batch_normalization_3/moving_variance:0\n",
      "model_weights/batch_normalization_30\n",
      "    weight_names: [b'batch_normalization_30/gamma:0' b'batch_normalization_30/beta:0'\n",
      " b'batch_normalization_30/moving_mean:0'\n",
      " b'batch_normalization_30/moving_variance:0']\n",
      "model_weights/batch_normalization_30/batch_normalization_30\n",
      "model_weights/batch_normalization_30/batch_normalization_30/beta:0\n",
      "model_weights/batch_normalization_30/batch_normalization_30/gamma:0\n",
      "model_weights/batch_normalization_30/batch_normalization_30/moving_mean:0\n",
      "model_weights/batch_normalization_30/batch_normalization_30/moving_variance:0\n",
      "model_weights/batch_normalization_31\n",
      "    weight_names: [b'batch_normalization_31/gamma:0' b'batch_normalization_31/beta:0'\n",
      " b'batch_normalization_31/moving_mean:0'\n",
      " b'batch_normalization_31/moving_variance:0']\n",
      "model_weights/batch_normalization_31/batch_normalization_31\n",
      "model_weights/batch_normalization_31/batch_normalization_31/beta:0\n",
      "model_weights/batch_normalization_31/batch_normalization_31/gamma:0\n",
      "model_weights/batch_normalization_31/batch_normalization_31/moving_mean:0\n",
      "model_weights/batch_normalization_31/batch_normalization_31/moving_variance:0\n",
      "model_weights/batch_normalization_32\n",
      "    weight_names: [b'batch_normalization_32/gamma:0' b'batch_normalization_32/beta:0'\n",
      " b'batch_normalization_32/moving_mean:0'\n",
      " b'batch_normalization_32/moving_variance:0']\n",
      "model_weights/batch_normalization_32/batch_normalization_32\n",
      "model_weights/batch_normalization_32/batch_normalization_32/beta:0\n",
      "model_weights/batch_normalization_32/batch_normalization_32/gamma:0\n",
      "model_weights/batch_normalization_32/batch_normalization_32/moving_mean:0\n",
      "model_weights/batch_normalization_32/batch_normalization_32/moving_variance:0\n",
      "model_weights/batch_normalization_33\n",
      "    weight_names: [b'batch_normalization_33/gamma:0' b'batch_normalization_33/beta:0'\n",
      " b'batch_normalization_33/moving_mean:0'\n",
      " b'batch_normalization_33/moving_variance:0']\n",
      "model_weights/batch_normalization_33/batch_normalization_33\n",
      "model_weights/batch_normalization_33/batch_normalization_33/beta:0\n",
      "model_weights/batch_normalization_33/batch_normalization_33/gamma:0\n",
      "model_weights/batch_normalization_33/batch_normalization_33/moving_mean:0\n",
      "model_weights/batch_normalization_33/batch_normalization_33/moving_variance:0\n",
      "model_weights/batch_normalization_34\n",
      "    weight_names: [b'batch_normalization_34/gamma:0' b'batch_normalization_34/beta:0'\n",
      " b'batch_normalization_34/moving_mean:0'\n",
      " b'batch_normalization_34/moving_variance:0']\n",
      "model_weights/batch_normalization_34/batch_normalization_34\n",
      "model_weights/batch_normalization_34/batch_normalization_34/beta:0\n",
      "model_weights/batch_normalization_34/batch_normalization_34/gamma:0\n",
      "model_weights/batch_normalization_34/batch_normalization_34/moving_mean:0\n",
      "model_weights/batch_normalization_34/batch_normalization_34/moving_variance:0\n",
      "model_weights/batch_normalization_35\n",
      "    weight_names: [b'batch_normalization_35/gamma:0' b'batch_normalization_35/beta:0'\n",
      " b'batch_normalization_35/moving_mean:0'\n",
      " b'batch_normalization_35/moving_variance:0']\n",
      "model_weights/batch_normalization_35/batch_normalization_35\n",
      "model_weights/batch_normalization_35/batch_normalization_35/beta:0\n",
      "model_weights/batch_normalization_35/batch_normalization_35/gamma:0\n",
      "model_weights/batch_normalization_35/batch_normalization_35/moving_mean:0\n",
      "model_weights/batch_normalization_35/batch_normalization_35/moving_variance:0\n",
      "model_weights/batch_normalization_4\n",
      "    weight_names: [b'batch_normalization_4/gamma:0' b'batch_normalization_4/beta:0'\n",
      " b'batch_normalization_4/moving_mean:0'\n",
      " b'batch_normalization_4/moving_variance:0']\n",
      "model_weights/batch_normalization_4/batch_normalization_4\n",
      "model_weights/batch_normalization_4/batch_normalization_4/beta:0\n",
      "model_weights/batch_normalization_4/batch_normalization_4/gamma:0\n",
      "model_weights/batch_normalization_4/batch_normalization_4/moving_mean:0\n",
      "model_weights/batch_normalization_4/batch_normalization_4/moving_variance:0\n",
      "model_weights/batch_normalization_5\n",
      "    weight_names: [b'batch_normalization_5/gamma:0' b'batch_normalization_5/beta:0'\n",
      " b'batch_normalization_5/moving_mean:0'\n",
      " b'batch_normalization_5/moving_variance:0']\n",
      "model_weights/batch_normalization_5/batch_normalization_5\n",
      "model_weights/batch_normalization_5/batch_normalization_5/beta:0\n",
      "model_weights/batch_normalization_5/batch_normalization_5/gamma:0\n",
      "model_weights/batch_normalization_5/batch_normalization_5/moving_mean:0\n",
      "model_weights/batch_normalization_5/batch_normalization_5/moving_variance:0\n",
      "model_weights/batch_normalization_6\n",
      "    weight_names: [b'batch_normalization_6/gamma:0' b'batch_normalization_6/beta:0'\n",
      " b'batch_normalization_6/moving_mean:0'\n",
      " b'batch_normalization_6/moving_variance:0']\n",
      "model_weights/batch_normalization_6/batch_normalization_6\n",
      "model_weights/batch_normalization_6/batch_normalization_6/beta:0\n",
      "model_weights/batch_normalization_6/batch_normalization_6/gamma:0\n",
      "model_weights/batch_normalization_6/batch_normalization_6/moving_mean:0\n",
      "model_weights/batch_normalization_6/batch_normalization_6/moving_variance:0\n",
      "model_weights/batch_normalization_7\n",
      "    weight_names: [b'batch_normalization_7/gamma:0' b'batch_normalization_7/beta:0'\n",
      " b'batch_normalization_7/moving_mean:0'\n",
      " b'batch_normalization_7/moving_variance:0']\n",
      "model_weights/batch_normalization_7/batch_normalization_7\n",
      "model_weights/batch_normalization_7/batch_normalization_7/beta:0\n",
      "model_weights/batch_normalization_7/batch_normalization_7/gamma:0\n",
      "model_weights/batch_normalization_7/batch_normalization_7/moving_mean:0\n",
      "model_weights/batch_normalization_7/batch_normalization_7/moving_variance:0\n",
      "model_weights/batch_normalization_8\n",
      "    weight_names: [b'batch_normalization_8/gamma:0' b'batch_normalization_8/beta:0'\n",
      " b'batch_normalization_8/moving_mean:0'\n",
      " b'batch_normalization_8/moving_variance:0']\n",
      "model_weights/batch_normalization_8/batch_normalization_8\n",
      "model_weights/batch_normalization_8/batch_normalization_8/beta:0\n",
      "model_weights/batch_normalization_8/batch_normalization_8/gamma:0\n",
      "model_weights/batch_normalization_8/batch_normalization_8/moving_mean:0\n",
      "model_weights/batch_normalization_8/batch_normalization_8/moving_variance:0\n",
      "model_weights/batch_normalization_9\n",
      "    weight_names: [b'batch_normalization_9/gamma:0' b'batch_normalization_9/beta:0'\n",
      " b'batch_normalization_9/moving_mean:0'\n",
      " b'batch_normalization_9/moving_variance:0']\n",
      "model_weights/batch_normalization_9/batch_normalization_9\n",
      "model_weights/batch_normalization_9/batch_normalization_9/beta:0\n",
      "model_weights/batch_normalization_9/batch_normalization_9/gamma:0\n",
      "model_weights/batch_normalization_9/batch_normalization_9/moving_mean:0\n",
      "model_weights/batch_normalization_9/batch_normalization_9/moving_variance:0\n",
      "model_weights/conv2d\n",
      "    weight_names: [b'conv2d/kernel:0' b'conv2d/bias:0']\n",
      "model_weights/conv2d/conv2d\n",
      "model_weights/conv2d/conv2d/bias:0\n",
      "model_weights/conv2d/conv2d/kernel:0\n",
      "model_weights/conv2d_1\n",
      "    weight_names: [b'conv2d_1/kernel:0' b'conv2d_1/bias:0']\n",
      "model_weights/conv2d_1/conv2d_1\n",
      "model_weights/conv2d_1/conv2d_1/bias:0\n",
      "model_weights/conv2d_1/conv2d_1/kernel:0\n",
      "model_weights/conv2d_2\n",
      "    weight_names: [b'conv2d_2/kernel:0' b'conv2d_2/bias:0']\n",
      "model_weights/conv2d_2/conv2d_2\n",
      "model_weights/conv2d_2/conv2d_2/bias:0\n",
      "model_weights/conv2d_2/conv2d_2/kernel:0\n",
      "model_weights/conv2d_3\n",
      "    weight_names: [b'conv2d_3/kernel:0' b'conv2d_3/bias:0']\n",
      "model_weights/conv2d_3/conv2d_3\n",
      "model_weights/conv2d_3/conv2d_3/bias:0\n",
      "model_weights/conv2d_3/conv2d_3/kernel:0\n",
      "model_weights/conv2d_4\n",
      "    weight_names: [b'conv2d_4/kernel:0' b'conv2d_4/bias:0']\n",
      "model_weights/conv2d_4/conv2d_4\n",
      "model_weights/conv2d_4/conv2d_4/bias:0\n",
      "model_weights/conv2d_4/conv2d_4/kernel:0\n",
      "model_weights/conv2d_5\n",
      "    weight_names: [b'conv2d_5/kernel:0' b'conv2d_5/bias:0']\n",
      "model_weights/conv2d_5/conv2d_5\n",
      "model_weights/conv2d_5/conv2d_5/bias:0\n",
      "model_weights/conv2d_5/conv2d_5/kernel:0\n",
      "model_weights/dense\n",
      "    weight_names: [b'dense/kernel:0' b'dense/bias:0']\n",
      "model_weights/dense/dense\n",
      "model_weights/dense/dense/bias:0\n",
      "model_weights/dense/dense/kernel:0\n",
      "model_weights/global_average_pooling2d\n",
      "    weight_names: []\n",
      "model_weights/input_1\n",
      "    weight_names: []\n",
      "model_weights/max_pooling2d\n",
      "    weight_names: []\n",
      "model_weights/max_pooling2d_1\n",
      "    weight_names: []\n",
      "model_weights/max_pooling2d_2\n",
      "    weight_names: []\n",
      "model_weights/max_pooling2d_3\n",
      "    weight_names: []\n",
      "model_weights/separable_conv2d\n",
      "    weight_names: [b'separable_conv2d/depthwise_kernel:0'\n",
      " b'separable_conv2d/pointwise_kernel:0' b'separable_conv2d/bias:0']\n",
      "model_weights/separable_conv2d/separable_conv2d\n",
      "model_weights/separable_conv2d/separable_conv2d/bias:0\n",
      "model_weights/separable_conv2d/separable_conv2d/depthwise_kernel:0\n",
      "model_weights/separable_conv2d/separable_conv2d/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_1\n",
      "    weight_names: [b'separable_conv2d_1/depthwise_kernel:0'\n",
      " b'separable_conv2d_1/pointwise_kernel:0' b'separable_conv2d_1/bias:0']\n",
      "model_weights/separable_conv2d_1/separable_conv2d_1\n",
      "model_weights/separable_conv2d_1/separable_conv2d_1/bias:0\n",
      "model_weights/separable_conv2d_1/separable_conv2d_1/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_1/separable_conv2d_1/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_10\n",
      "    weight_names: [b'separable_conv2d_10/depthwise_kernel:0'\n",
      " b'separable_conv2d_10/pointwise_kernel:0' b'separable_conv2d_10/bias:0']\n",
      "model_weights/separable_conv2d_10/separable_conv2d_10\n",
      "model_weights/separable_conv2d_10/separable_conv2d_10/bias:0\n",
      "model_weights/separable_conv2d_10/separable_conv2d_10/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_10/separable_conv2d_10/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_11\n",
      "    weight_names: [b'separable_conv2d_11/depthwise_kernel:0'\n",
      " b'separable_conv2d_11/pointwise_kernel:0' b'separable_conv2d_11/bias:0']\n",
      "model_weights/separable_conv2d_11/separable_conv2d_11\n",
      "model_weights/separable_conv2d_11/separable_conv2d_11/bias:0\n",
      "model_weights/separable_conv2d_11/separable_conv2d_11/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_11/separable_conv2d_11/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_12\n",
      "    weight_names: [b'separable_conv2d_12/depthwise_kernel:0'\n",
      " b'separable_conv2d_12/pointwise_kernel:0' b'separable_conv2d_12/bias:0']\n",
      "model_weights/separable_conv2d_12/separable_conv2d_12\n",
      "model_weights/separable_conv2d_12/separable_conv2d_12/bias:0\n",
      "model_weights/separable_conv2d_12/separable_conv2d_12/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_12/separable_conv2d_12/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_13\n",
      "    weight_names: [b'separable_conv2d_13/depthwise_kernel:0'\n",
      " b'separable_conv2d_13/pointwise_kernel:0' b'separable_conv2d_13/bias:0']\n",
      "model_weights/separable_conv2d_13/separable_conv2d_13\n",
      "model_weights/separable_conv2d_13/separable_conv2d_13/bias:0\n",
      "model_weights/separable_conv2d_13/separable_conv2d_13/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_13/separable_conv2d_13/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_14\n",
      "    weight_names: [b'separable_conv2d_14/depthwise_kernel:0'\n",
      " b'separable_conv2d_14/pointwise_kernel:0' b'separable_conv2d_14/bias:0']\n",
      "model_weights/separable_conv2d_14/separable_conv2d_14\n",
      "model_weights/separable_conv2d_14/separable_conv2d_14/bias:0\n",
      "model_weights/separable_conv2d_14/separable_conv2d_14/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_14/separable_conv2d_14/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_15\n",
      "    weight_names: [b'separable_conv2d_15/depthwise_kernel:0'\n",
      " b'separable_conv2d_15/pointwise_kernel:0' b'separable_conv2d_15/bias:0']\n",
      "model_weights/separable_conv2d_15/separable_conv2d_15\n",
      "model_weights/separable_conv2d_15/separable_conv2d_15/bias:0\n",
      "model_weights/separable_conv2d_15/separable_conv2d_15/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_15/separable_conv2d_15/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_16\n",
      "    weight_names: [b'separable_conv2d_16/depthwise_kernel:0'\n",
      " b'separable_conv2d_16/pointwise_kernel:0' b'separable_conv2d_16/bias:0']\n",
      "model_weights/separable_conv2d_16/separable_conv2d_16\n",
      "model_weights/separable_conv2d_16/separable_conv2d_16/bias:0\n",
      "model_weights/separable_conv2d_16/separable_conv2d_16/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_16/separable_conv2d_16/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_17\n",
      "    weight_names: [b'separable_conv2d_17/depthwise_kernel:0'\n",
      " b'separable_conv2d_17/pointwise_kernel:0' b'separable_conv2d_17/bias:0']\n",
      "model_weights/separable_conv2d_17/separable_conv2d_17\n",
      "model_weights/separable_conv2d_17/separable_conv2d_17/bias:0\n",
      "model_weights/separable_conv2d_17/separable_conv2d_17/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_17/separable_conv2d_17/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_18\n",
      "    weight_names: [b'separable_conv2d_18/depthwise_kernel:0'\n",
      " b'separable_conv2d_18/pointwise_kernel:0' b'separable_conv2d_18/bias:0']\n",
      "model_weights/separable_conv2d_18/separable_conv2d_18\n",
      "model_weights/separable_conv2d_18/separable_conv2d_18/bias:0\n",
      "model_weights/separable_conv2d_18/separable_conv2d_18/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_18/separable_conv2d_18/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_19\n",
      "    weight_names: [b'separable_conv2d_19/depthwise_kernel:0'\n",
      " b'separable_conv2d_19/pointwise_kernel:0' b'separable_conv2d_19/bias:0']\n",
      "model_weights/separable_conv2d_19/separable_conv2d_19\n",
      "model_weights/separable_conv2d_19/separable_conv2d_19/bias:0\n",
      "model_weights/separable_conv2d_19/separable_conv2d_19/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_19/separable_conv2d_19/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_2\n",
      "    weight_names: [b'separable_conv2d_2/depthwise_kernel:0'\n",
      " b'separable_conv2d_2/pointwise_kernel:0' b'separable_conv2d_2/bias:0']\n",
      "model_weights/separable_conv2d_2/separable_conv2d_2\n",
      "model_weights/separable_conv2d_2/separable_conv2d_2/bias:0\n",
      "model_weights/separable_conv2d_2/separable_conv2d_2/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_2/separable_conv2d_2/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_20\n",
      "    weight_names: [b'separable_conv2d_20/depthwise_kernel:0'\n",
      " b'separable_conv2d_20/pointwise_kernel:0' b'separable_conv2d_20/bias:0']\n",
      "model_weights/separable_conv2d_20/separable_conv2d_20\n",
      "model_weights/separable_conv2d_20/separable_conv2d_20/bias:0\n",
      "model_weights/separable_conv2d_20/separable_conv2d_20/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_20/separable_conv2d_20/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_21\n",
      "    weight_names: [b'separable_conv2d_21/depthwise_kernel:0'\n",
      " b'separable_conv2d_21/pointwise_kernel:0' b'separable_conv2d_21/bias:0']\n",
      "model_weights/separable_conv2d_21/separable_conv2d_21\n",
      "model_weights/separable_conv2d_21/separable_conv2d_21/bias:0\n",
      "model_weights/separable_conv2d_21/separable_conv2d_21/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_21/separable_conv2d_21/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_22\n",
      "    weight_names: [b'separable_conv2d_22/depthwise_kernel:0'\n",
      " b'separable_conv2d_22/pointwise_kernel:0' b'separable_conv2d_22/bias:0']\n",
      "model_weights/separable_conv2d_22/separable_conv2d_22\n",
      "model_weights/separable_conv2d_22/separable_conv2d_22/bias:0\n",
      "model_weights/separable_conv2d_22/separable_conv2d_22/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_22/separable_conv2d_22/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_23\n",
      "    weight_names: [b'separable_conv2d_23/depthwise_kernel:0'\n",
      " b'separable_conv2d_23/pointwise_kernel:0' b'separable_conv2d_23/bias:0']\n",
      "model_weights/separable_conv2d_23/separable_conv2d_23\n",
      "model_weights/separable_conv2d_23/separable_conv2d_23/bias:0\n",
      "model_weights/separable_conv2d_23/separable_conv2d_23/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_23/separable_conv2d_23/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_24\n",
      "    weight_names: [b'separable_conv2d_24/depthwise_kernel:0'\n",
      " b'separable_conv2d_24/pointwise_kernel:0' b'separable_conv2d_24/bias:0']\n",
      "model_weights/separable_conv2d_24/separable_conv2d_24\n",
      "model_weights/separable_conv2d_24/separable_conv2d_24/bias:0\n",
      "model_weights/separable_conv2d_24/separable_conv2d_24/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_24/separable_conv2d_24/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_25\n",
      "    weight_names: [b'separable_conv2d_25/depthwise_kernel:0'\n",
      " b'separable_conv2d_25/pointwise_kernel:0' b'separable_conv2d_25/bias:0']\n",
      "model_weights/separable_conv2d_25/separable_conv2d_25\n",
      "model_weights/separable_conv2d_25/separable_conv2d_25/bias:0\n",
      "model_weights/separable_conv2d_25/separable_conv2d_25/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_25/separable_conv2d_25/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_26\n",
      "    weight_names: [b'separable_conv2d_26/depthwise_kernel:0'\n",
      " b'separable_conv2d_26/pointwise_kernel:0' b'separable_conv2d_26/bias:0']\n",
      "model_weights/separable_conv2d_26/separable_conv2d_26\n",
      "model_weights/separable_conv2d_26/separable_conv2d_26/bias:0\n",
      "model_weights/separable_conv2d_26/separable_conv2d_26/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_26/separable_conv2d_26/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_27\n",
      "    weight_names: [b'separable_conv2d_27/depthwise_kernel:0'\n",
      " b'separable_conv2d_27/pointwise_kernel:0' b'separable_conv2d_27/bias:0']\n",
      "model_weights/separable_conv2d_27/separable_conv2d_27\n",
      "model_weights/separable_conv2d_27/separable_conv2d_27/bias:0\n",
      "model_weights/separable_conv2d_27/separable_conv2d_27/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_27/separable_conv2d_27/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_28\n",
      "    weight_names: [b'separable_conv2d_28/depthwise_kernel:0'\n",
      " b'separable_conv2d_28/pointwise_kernel:0' b'separable_conv2d_28/bias:0']\n",
      "model_weights/separable_conv2d_28/separable_conv2d_28\n",
      "model_weights/separable_conv2d_28/separable_conv2d_28/bias:0\n",
      "model_weights/separable_conv2d_28/separable_conv2d_28/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_28/separable_conv2d_28/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_29\n",
      "    weight_names: [b'separable_conv2d_29/depthwise_kernel:0'\n",
      " b'separable_conv2d_29/pointwise_kernel:0' b'separable_conv2d_29/bias:0']\n",
      "model_weights/separable_conv2d_29/separable_conv2d_29\n",
      "model_weights/separable_conv2d_29/separable_conv2d_29/bias:0\n",
      "model_weights/separable_conv2d_29/separable_conv2d_29/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_29/separable_conv2d_29/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_3\n",
      "    weight_names: [b'separable_conv2d_3/depthwise_kernel:0'\n",
      " b'separable_conv2d_3/pointwise_kernel:0' b'separable_conv2d_3/bias:0']\n",
      "model_weights/separable_conv2d_3/separable_conv2d_3\n",
      "model_weights/separable_conv2d_3/separable_conv2d_3/bias:0\n",
      "model_weights/separable_conv2d_3/separable_conv2d_3/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_3/separable_conv2d_3/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_30\n",
      "    weight_names: [b'separable_conv2d_30/depthwise_kernel:0'\n",
      " b'separable_conv2d_30/pointwise_kernel:0' b'separable_conv2d_30/bias:0']\n",
      "model_weights/separable_conv2d_30/separable_conv2d_30\n",
      "model_weights/separable_conv2d_30/separable_conv2d_30/bias:0\n",
      "model_weights/separable_conv2d_30/separable_conv2d_30/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_30/separable_conv2d_30/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_31\n",
      "    weight_names: [b'separable_conv2d_31/depthwise_kernel:0'\n",
      " b'separable_conv2d_31/pointwise_kernel:0' b'separable_conv2d_31/bias:0']\n",
      "model_weights/separable_conv2d_31/separable_conv2d_31\n",
      "model_weights/separable_conv2d_31/separable_conv2d_31/bias:0\n",
      "model_weights/separable_conv2d_31/separable_conv2d_31/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_31/separable_conv2d_31/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_32\n",
      "    weight_names: [b'separable_conv2d_32/depthwise_kernel:0'\n",
      " b'separable_conv2d_32/pointwise_kernel:0' b'separable_conv2d_32/bias:0']\n",
      "model_weights/separable_conv2d_32/separable_conv2d_32\n",
      "model_weights/separable_conv2d_32/separable_conv2d_32/bias:0\n",
      "model_weights/separable_conv2d_32/separable_conv2d_32/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_32/separable_conv2d_32/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_33\n",
      "    weight_names: [b'separable_conv2d_33/depthwise_kernel:0'\n",
      " b'separable_conv2d_33/pointwise_kernel:0' b'separable_conv2d_33/bias:0']\n",
      "model_weights/separable_conv2d_33/separable_conv2d_33\n",
      "model_weights/separable_conv2d_33/separable_conv2d_33/bias:0\n",
      "model_weights/separable_conv2d_33/separable_conv2d_33/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_33/separable_conv2d_33/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_4\n",
      "    weight_names: [b'separable_conv2d_4/depthwise_kernel:0'\n",
      " b'separable_conv2d_4/pointwise_kernel:0' b'separable_conv2d_4/bias:0']\n",
      "model_weights/separable_conv2d_4/separable_conv2d_4\n",
      "model_weights/separable_conv2d_4/separable_conv2d_4/bias:0\n",
      "model_weights/separable_conv2d_4/separable_conv2d_4/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_4/separable_conv2d_4/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_5\n",
      "    weight_names: [b'separable_conv2d_5/depthwise_kernel:0'\n",
      " b'separable_conv2d_5/pointwise_kernel:0' b'separable_conv2d_5/bias:0']\n",
      "model_weights/separable_conv2d_5/separable_conv2d_5\n",
      "model_weights/separable_conv2d_5/separable_conv2d_5/bias:0\n",
      "model_weights/separable_conv2d_5/separable_conv2d_5/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_5/separable_conv2d_5/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_6\n",
      "    weight_names: [b'separable_conv2d_6/depthwise_kernel:0'\n",
      " b'separable_conv2d_6/pointwise_kernel:0' b'separable_conv2d_6/bias:0']\n",
      "model_weights/separable_conv2d_6/separable_conv2d_6\n",
      "model_weights/separable_conv2d_6/separable_conv2d_6/bias:0\n",
      "model_weights/separable_conv2d_6/separable_conv2d_6/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_6/separable_conv2d_6/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_7\n",
      "    weight_names: [b'separable_conv2d_7/depthwise_kernel:0'\n",
      " b'separable_conv2d_7/pointwise_kernel:0' b'separable_conv2d_7/bias:0']\n",
      "model_weights/separable_conv2d_7/separable_conv2d_7\n",
      "model_weights/separable_conv2d_7/separable_conv2d_7/bias:0\n",
      "model_weights/separable_conv2d_7/separable_conv2d_7/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_7/separable_conv2d_7/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_8\n",
      "    weight_names: [b'separable_conv2d_8/depthwise_kernel:0'\n",
      " b'separable_conv2d_8/pointwise_kernel:0' b'separable_conv2d_8/bias:0']\n",
      "model_weights/separable_conv2d_8/separable_conv2d_8\n",
      "model_weights/separable_conv2d_8/separable_conv2d_8/bias:0\n",
      "model_weights/separable_conv2d_8/separable_conv2d_8/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_8/separable_conv2d_8/pointwise_kernel:0\n",
      "model_weights/separable_conv2d_9\n",
      "    weight_names: [b'separable_conv2d_9/depthwise_kernel:0'\n",
      " b'separable_conv2d_9/pointwise_kernel:0' b'separable_conv2d_9/bias:0']\n",
      "model_weights/separable_conv2d_9/separable_conv2d_9\n",
      "model_weights/separable_conv2d_9/separable_conv2d_9/bias:0\n",
      "model_weights/separable_conv2d_9/separable_conv2d_9/depthwise_kernel:0\n",
      "model_weights/separable_conv2d_9/separable_conv2d_9/pointwise_kernel:0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "edf4f9447fe1678b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:55:09.205167Z",
     "start_time": "2024-07-09T19:55:09.202615Z"
    }
   },
   "source": [
    "# FINAL, Test success\n",
    "print(\"The following cell has the integrated code which is final one of version 1\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following cell has the integrated code which is final one of version 1\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "0e8bd206-752f-4206-a47c-a881219c9b2f",
   "metadata": {},
   "source": [
    "!pip3 install opencv-python-headless\n",
    "!pip3 install dlib\n",
    "!pip3 install numpy\n",
    "!pip3 install ipython\n",
    "!pip3 install scipy\n",
    "!pip3 install imutils\n",
    "!pip3 install tensorflow\n",
    "!pip3 install mediapipe\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from scipy.ndimage import zoom\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "\n",
    "# Custom deserialization function for VarianceScaling\n",
    "def custom_VarianceScaling_deserializer(config):\n",
    "    from tensorflow.keras.initializers import VarianceScaling\n",
    "    # Remove 'dtype' from config if it exists\n",
    "    config.pop('dtype', None)\n",
    "    return VarianceScaling(**config)\n",
    "\n",
    "# Register the custom deserializer\n",
    "tf.keras.utils.get_custom_objects().update({'VarianceScaling': custom_VarianceScaling_deserializer})\n",
    "\n",
    "# Function to detect eyes in a frame\n",
    "def detect_eyes(frame, shape):\n",
    "    left_eye = shape[36:42]\n",
    "    right_eye = shape[42:48]\n",
    "    return left_eye, right_eye\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    eye = np.array([(point[0], point[1]) for point in eye])\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Function to get the specified file's path\n",
    "def get_abs_path(directory, file):\n",
    "    directory_path = os.path.join(os.getcwd(), '..', directory)\n",
    "    file_path = os.path.join(directory_path, file)\n",
    "    return file_path\n",
    "\n",
    "# Load dlib face detector and facial landmarks predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(get_abs_path('scripts', 'shape_predictor_68_face_landmarks.dat'))\n",
    "\n",
    "# Load emotion detection model\n",
    "emotion_model = load_model(get_abs_path('scripts', 'FER_model.h5'))\n",
    "\n",
    "emotion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Initialize video capture from the camera\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you can change it if you have multiple cameras)\n",
    "\n",
    "# Get video properties for the output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize video writer for the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter(get_abs_path('results', 'outputvideo.avi'), fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize variables to record durations\n",
    "duration_eyes_closed = {}\n",
    "duration_looking_left = {}\n",
    "duration_looking_right = {}\n",
    "duration_looking_straight = {}\n",
    "\n",
    "# Initialize variables for counting eye movement\n",
    "count_left = {}\n",
    "count_right = {}\n",
    "count_straight = {}\n",
    "\n",
    "# Variables to track emotion detected\n",
    "emotion_start_time = time.time()\n",
    "emotion_duration = {\"angry\": {}, \"sad\": {}, \"happy\": {}, \"fear\": {}, \"disgust\": {}, \"neutral\": {}, \"surprise\": {}}\n",
    "\n",
    "# Variables to track time spent in different head pose directions\n",
    "time_forward_seconds = {}\n",
    "time_left_seconds = {}\n",
    "time_right_seconds = {}\n",
    "time_up_seconds = {}\n",
    "time_down_seconds = {}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        person_id = f\"Person {i+1}\"\n",
    "\n",
    "        if person_id not in duration_eyes_closed:\n",
    "            duration_eyes_closed[person_id] = 0\n",
    "            duration_looking_left[person_id] = 0\n",
    "            duration_looking_right[person_id] = 0\n",
    "            duration_looking_straight[person_id] = 0\n",
    "            count_left[person_id] = 0\n",
    "            count_right[person_id] = 0\n",
    "            count_straight[person_id] = 0\n",
    "            time_forward_seconds[person_id] = 0\n",
    "            time_left_seconds[person_id] = 0\n",
    "            time_right_seconds[person_id] = 0\n",
    "            time_up_seconds[person_id] = 0\n",
    "            time_down_seconds[person_id] = 0\n",
    "            for emotion in emotion_duration:\n",
    "                emotion_duration[emotion][person_id] = 0\n",
    "\n",
    "        # Eye tracking\n",
    "        left_eye, right_eye = detect_eyes(frame, shape)\n",
    "\n",
    "        if left_eye is not None and right_eye is not None:\n",
    "            ear_left = calculate_ear(left_eye)\n",
    "            ear_right = calculate_ear(right_eye)\n",
    "\n",
    "            # Calculate the average EAR for both eyes\n",
    "            avg_ear = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            # Set a threshold for distraction detection (you may need to adjust this)\n",
    "            distraction_threshold = 0.2\n",
    "\n",
    "            # Check if the person is distracted\n",
    "            if avg_ear < distraction_threshold:\n",
    "                cv2.putText(frame, f\"{person_id}: Eyes Closed\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 2)\n",
    "                duration_eyes_closed[person_id] += 1 / fps  # Increment the duration\n",
    "                count_straight[person_id] += 1\n",
    "\n",
    "            else:\n",
    "                # Check gaze direction\n",
    "                horizontal_ratio = (left_eye[0][0] + right_eye[3][0]) / 2 / width\n",
    "                if horizontal_ratio < 0.4:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Left\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_left[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_left[person_id] += 1\n",
    "                elif horizontal_ratio > 0.6:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Right\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_right[person_id] += 1 / fps  # Increment the duration\n",
    "                    count_right[person_id] += 1\n",
    "                else:\n",
    "                    cv2.putText(frame, f\"{person_id}: Looking Straight\", (10, 30 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "                    duration_looking_straight[person_id] += 1 / fps  # Increment the duration\n",
    "\n",
    "            # Draw contours around eyes\n",
    "            for eye in [left_eye, right_eye]:\n",
    "                for point in eye:\n",
    "                    x, y = point[0], point[1]\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "        # Emotion detection\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        face_crop = gray[y:y + h, x:x + w]\n",
    "        face_crop = zoom(face_crop, (48 / face_crop.shape[0], 48 / face_crop.shape[1]))\n",
    "        face_crop = face_crop.astype(np.float32)\n",
    "        face_crop /= float(face_crop.max())\n",
    "        face_crop = np.reshape(face_crop.flatten(), (1, 48, 48, 1))\n",
    "\n",
    "        prediction = emotion_model.predict(face_crop)\n",
    "        prediction_result = np.argmax(prediction)\n",
    "\n",
    "        # Rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Annotate main image with emotion label\n",
    "        emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "        emotion_label = emotion_labels[prediction_result]\n",
    "        cv2.putText(frame, f\"{person_id}: {emotion_label}\", (x + w - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        emotion_duration[emotion_label.lower()][person_id] += time.time() - emotion_start_time\n",
    "        emotion_start_time = time.time()\n",
    "\n",
    "        # Head pose estimation\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb.flags.writeable = False\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        frame_rgb.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        img_h, img_w, img_c = frame.shape\n",
    "        face_3d = []\n",
    "        face_2d = []\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "          for face_landmarks in results.multi_face_landmarks:\n",
    "              for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                  if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                      if idx == 1:\n",
    "                          nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                          nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 8000)\n",
    "                      x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                      # Get the 2D Coordinates\n",
    "                      face_2d.append([x, y])\n",
    "\n",
    "                      # Get the 3D Coordinates\n",
    "                      face_3d.append([x, y, lm.z])\n",
    "\n",
    "          face_2d = np.array(face_2d, dtype=np.float64)\n",
    "          face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "          # Camera matrix\n",
    "          focal_length = 1 * img_w\n",
    "          cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                [0, focal_length, img_h / 2],\n",
    "                                [0, 0, 1]])\n",
    "\n",
    "          # Distortion parameters\n",
    "          dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "          # Solve PnP\n",
    "          success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "          # Get rotational matrix\n",
    "          rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "          # Get angles\n",
    "          angles, mtx_r, mtx_q, qx, qy, qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "          # Get the y rotation degree\n",
    "          x_angle = angles[0] * 360\n",
    "          y_angle = angles[1] * 360\n",
    "          z_angle = angles[2] * 360\n",
    "\n",
    "          # See where the user's head tilting\n",
    "          if y_angle < -10:\n",
    "              text = \"Looking Left\"\n",
    "              time_left_seconds[person_id] += 1 / fps\n",
    "          elif y_angle > 10:\n",
    "              text = \"Looking Right\"\n",
    "              time_right_seconds[person_id] += 1 / fps\n",
    "          elif x_angle < -10:\n",
    "              text = \"Looking Down\"\n",
    "              time_down_seconds[person_id] += 1 / fps\n",
    "          elif x_angle > 10:\n",
    "              text = \"Looking Up\"\n",
    "              time_up_seconds[person_id] += 1 / fps\n",
    "          else:\n",
    "              text = \"Looking Forward\"\n",
    "              time_forward_seconds[person_id] += 1 / fps\n",
    "\n",
    "          # Display the text\n",
    "          cv2.putText(frame, f\"{person_id}: {text}\", (500, 50 + i * 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Clear the output in Jupyter notebook\n",
    "    clear_output(wait=True)\n",
    "    display(frame)\n",
    "\n",
    "    # Exit the loop when the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "with open(get_abs_path('results', 'eye_tracking_data.csv'), 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Duration Eyes Closed (s)\", \"Duration Looking Left (s)\", \"Duration Looking Right (s)\", \"Duration Looking Straight (s)\", \"Left Counts\", \"Right Counts\", \"Straight Counts\"])\n",
    "    for person_id in duration_eyes_closed:\n",
    "        writer.writerow([person_id, duration_eyes_closed[person_id], duration_looking_left[person_id], duration_looking_right[person_id], duration_looking_straight[person_id], count_left[person_id], count_right[person_id], count_straight[person_id]])\n",
    "\n",
    "with open(get_abs_path('results', 'emotion_detection_data.csv'), 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Angry (s)\", \"Sad (s)\", \"Happy (s)\", \"Fear (s)\", \"Disgust (s)\", \"Neutral (s)\", \"Surprise (s)\"])\n",
    "    for person_id in emotion_duration[\"angry\"]:\n",
    "        writer.writerow([person_id, emotion_duration[\"angry\"][person_id], emotion_duration[\"sad\"][person_id], emotion_duration[\"happy\"][person_id], emotion_duration[\"fear\"][person_id], emotion_duration[\"disgust\"][person_id], emotion_duration[\"neutral\"][person_id], emotion_duration[\"surprise\"][person_id]])\n",
    "\n",
    "with open(get_abs_path('results', 'head_pose_data.csv'), 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Person ID\", \"Looking Forward (s)\", \"Looking Left (s)\", \"Looking Right (s)\", \"Looking Up (s)\", \"Looking Down (s)\"])\n",
    "    for person_id in time_forward_seconds:\n",
    "        writer.writerow([person_id, time_forward_seconds[person_id], time_left_seconds[person_id], time_right_seconds[person_id], time_up_seconds[person_id], time_down_seconds[person_id]])\n"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 175\u001B[0m\n\u001B[1;32m    173\u001B[0m (x, y, w, h) \u001B[38;5;241m=\u001B[39m face_utils\u001B[38;5;241m.\u001B[39mrect_to_bb(face)\n\u001B[1;32m    174\u001B[0m face_crop \u001B[38;5;241m=\u001B[39m gray[y:y \u001B[38;5;241m+\u001B[39m h, x:x \u001B[38;5;241m+\u001B[39m w]\n\u001B[0;32m--> 175\u001B[0m face_crop \u001B[38;5;241m=\u001B[39m zoom(face_crop, (\u001B[38;5;241m48\u001B[39m \u001B[38;5;241m/\u001B[39m face_crop\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m48\u001B[39m \u001B[38;5;241m/\u001B[39m face_crop\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]))\n\u001B[1;32m    176\u001B[0m face_crop \u001B[38;5;241m=\u001B[39m face_crop\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m    177\u001B[0m face_crop \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(face_crop\u001B[38;5;241m.\u001B[39mmax())\n",
      "\u001B[0;31mZeroDivisionError\u001B[0m: division by zero"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f40b5-f2b6-4e79-82fc-a800afdc90eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
